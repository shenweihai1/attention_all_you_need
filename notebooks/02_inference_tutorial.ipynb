{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Inference Tutorial\n",
    "\n",
    "This notebook shows how to use a trained Transformer model for translation.\n",
    "Each step includes validation to verify correctness.\n",
    "\n",
    "## Overview\n",
    "1. Setup and imports\n",
    "2. Load the trained model\n",
    "3. Prepare input text\n",
    "4. Run inference (greedy decoding)\n",
    "5. Analyze attention weights\n",
    "6. Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory to path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import our Transformer implementation\n",
    "from src import Transformer\n",
    "from src.tokenizer import SimpleTokenizer, pad_sequences\n",
    "from src.attention import create_causal_mask, create_padding_mask\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Trained Model\n",
    "\n",
    "Load the checkpoint saved from training tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved checkpoint...\n",
      "\n",
      "============================================================\n",
      "VALIDATION: Model Configuration\n",
      "============================================================\n",
      "  vocab_size: 104\n",
      "  d_model: 128\n",
      "  n_heads: 4\n",
      "  n_layers: 2\n",
      "  d_ff: 256\n",
      "  dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if checkpoint exists, if not create a demo model\n",
    "checkpoint_path = '../checkpoints/demo_model.pt'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading saved checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    config = checkpoint['config']\n",
    "else:\n",
    "    print(\"No checkpoint found. Creating fresh model for demonstration...\")\n",
    "    config = {\n",
    "        'vocab_size': 100,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 4,\n",
    "        'n_layers': 2,\n",
    "        'd_ff': 256,\n",
    "        'dropout': 0.1,\n",
    "    }\n",
    "    checkpoint = None\n",
    "\n",
    "# Validate: Show configuration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION: Model Configuration\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Tokenizer Rebuilt\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 62\n",
      "BOS_ID: 2\n",
      "EOS_ID: 3\n",
      "PAD_ID: 0\n"
     ]
    }
   ],
   "source": [
    "# Rebuild tokenizer with same vocabulary\n",
    "# In practice, save and load tokenizer too\n",
    "src_sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"Hello world\",\n",
    "    \"How are you today\",\n",
    "    \"I love machine learning\",\n",
    "    \"The weather is nice\",\n",
    "    \"Good morning everyone\",\n",
    "    \"This is a test\",\n",
    "    \"The dog runs fast\",\n",
    "]\n",
    "\n",
    "tgt_sentences = [\n",
    "    \"Die Katze saß auf der Matte\",\n",
    "    \"Hallo Welt\",\n",
    "    \"Wie geht es dir heute\",\n",
    "    \"Ich liebe maschinelles Lernen\",\n",
    "    \"Das Wetter ist schön\",\n",
    "    \"Guten Morgen allerseits\",\n",
    "    \"Das ist ein Test\",\n",
    "    \"Der Hund läuft schnell\",\n",
    "]\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(src_sentences + tgt_sentences)\n",
    "\n",
    "# Update vocab_size to match tokenizer\n",
    "config['vocab_size'] = tokenizer.vocab_size\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Tokenizer Rebuilt\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"BOS_ID: {tokenizer.bos_id}\")\n",
    "print(f\"EOS_ID: {tokenizer.eos_id}\")\n",
    "print(f\"PAD_ID: {tokenizer.pad_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load weights (vocab size mismatch?): Error(s) in loading state_dict for Transformer:\n",
      "\tsize mismatch for src_embedding.embedding.weight: copying a param with shape torch.Size([104, 128]) from checkpoint, the shape in current model is torch.Size([62, 128]).\n",
      "\tsize mismatch for tgt_embedding.embedding.weight: copying a param with shape torch.Size([104, 128]) from checkpoint, the shape in current model is torch.Size([62, 128]).\n",
      "\tsize mismatch for positional_encoding.pe: copying a param with shape torch.Size([1, 100, 128]) from checkpoint, the shape in current model is torch.Size([1, 5000, 128]).\n",
      "\tsize mismatch for output_projection.weight: copying a param with shape torch.Size([104, 128]) from checkpoint, the shape in current model is torch.Size([62, 128]).\n",
      "\tsize mismatch for output_projection.bias: copying a param with shape torch.Size([104]) from checkpoint, the shape in current model is torch.Size([62]).\n",
      "Using fresh model weights.\n",
      "\n",
      "============================================================\n",
      "VALIDATION: Model Ready\n",
      "============================================================\n",
      "\n",
      "Model loaded and set to eval mode\n",
      "Total parameters: 683,838\n",
      "\n",
      "✓ Model is ready for inference!\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = Transformer(\n",
    "    src_vocab_size=config['vocab_size'],\n",
    "    tgt_vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_encoder_layers=config['n_layers'],\n",
    "    n_decoder_layers=config['n_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout=config['dropout'],\n",
    "    pad_idx=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "# Load weights if checkpoint exists\n",
    "if checkpoint is not None:\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Model weights loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load weights (vocab size mismatch?): {e}\")\n",
    "        print(\"Using fresh model weights.\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION: Model Ready\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel loaded and set to eval mode\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\n✓ Model is ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Input Text\n",
    "\n",
    "Tokenize the input sentence for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Input Preparation\n",
      "============================================================\n",
      "\n",
      "Input text: 'The cat sat on the mat'\n",
      "\n",
      "Token IDs: [2, 4, 31, 52, 50, 57, 47, 3]\n",
      "Tensor shape: torch.Size([1, 8])\n",
      "\n",
      "Token breakdown:\n",
      "  Position 0: ID=  2 -> ''\n",
      "  Position 1: ID=  4 -> 'The'\n",
      "  Position 2: ID= 31 -> 'cat'\n",
      "  Position 3: ID= 52 -> 'sat'\n",
      "  Position 4: ID= 50 -> 'on'\n",
      "  Position 5: ID= 57 -> 'the'\n",
      "  Position 6: ID= 47 -> 'mat'\n",
      "  Position 7: ID=  3 -> ''\n",
      "\n",
      "✓ Input prepared correctly!\n"
     ]
    }
   ],
   "source": [
    "# Input sentence to translate\n",
    "input_text = \"The cat sat on the mat\"\n",
    "\n",
    "# Tokenize\n",
    "src_ids = tokenizer.encode(input_text, add_bos=True, add_eos=True)\n",
    "src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "\n",
    "# Validate input\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Input Preparation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput text: '{input_text}'\")\n",
    "print(f\"\\nToken IDs: {src_ids}\")\n",
    "print(f\"Tensor shape: {src_tensor.shape}\")\n",
    "\n",
    "# Show token-by-token breakdown\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, token_id in enumerate(src_ids):\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    print(f\"  Position {i}: ID={token_id:3d} -> '{token_str}'\")\n",
    "\n",
    "print(f\"\\n✓ Input prepared correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Inference (Greedy Decoding)\n",
    "\n",
    "Generate translation using greedy decoding (selecting most probable token at each step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Greedy Decoding Results\n",
      "============================================================\n",
      "\n",
      "Input:  'The cat sat on the mat'\n",
      "Output: 'Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello'\n",
      "\n",
      "Generated IDs: [2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src, tokenizer, max_len=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Greedy decoding: select the most probable token at each step.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        src: Source token tensor (1, src_len)\n",
    "        tokenizer: Tokenizer with bos_id, eos_id\n",
    "        max_len: Maximum output length\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Generated token IDs and step-by-step info\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src = src.to(device)\n",
    "    memory = model.encode(src)\n",
    "    \n",
    "    # Start with BOS token\n",
    "    generated = [tokenizer.bos_id]\n",
    "    step_info = []\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        # Create target tensor\n",
    "        tgt = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "        \n",
    "        # Decode\n",
    "        tgt_mask = model._create_tgt_mask(tgt)\n",
    "        decoder_output = model.decode(tgt, memory, tgt_mask)\n",
    "        \n",
    "        # Get logits for last position\n",
    "        logits = model.output_projection(decoder_output[:, -1, :])\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Greedy: select most probable\n",
    "        next_token = probs.argmax(dim=-1).item()\n",
    "        \n",
    "        # Get top-3 for analysis\n",
    "        top_probs, top_ids = probs.topk(3, dim=-1)\n",
    "        \n",
    "        step_info.append({\n",
    "            'step': step + 1,\n",
    "            'selected': next_token,\n",
    "            'selected_prob': probs[0, next_token].item(),\n",
    "            'top3': [(top_ids[0, i].item(), top_probs[0, i].item()) for i in range(3)]\n",
    "        })\n",
    "        \n",
    "        # Append token\n",
    "        generated.append(next_token)\n",
    "        \n",
    "        # Stop if EOS\n",
    "        if next_token == tokenizer.eos_id:\n",
    "            break\n",
    "    \n",
    "    return generated, step_info\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    generated_ids, step_info = greedy_decode(\n",
    "        model, src_tensor, tokenizer, max_len=20, device=device\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Greedy Decoding Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput:  '{input_text}'\")\n",
    "print(f\"Output: '{output_text}'\")\n",
    "print(f\"\\nGenerated IDs: {generated_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Step-by-Step Decoding\n",
      "============================================================\n",
      "\n",
      "Decoding steps (first 5):\n",
      "\n",
      "  Step 1:\n",
      "    Selected: ID=13 ('Hello') prob=0.1277\n",
      "    Top 3 candidates:\n",
      "      ID= 13 ('Hello') prob=0.1277\n",
      "      ID= 22 ('Test') prob=0.0944\n",
      "      ID= 34 ('dog') prob=0.0756\n",
      "\n",
      "  Step 2:\n",
      "    Selected: ID=13 ('Hello') prob=0.1632\n",
      "    Top 3 candidates:\n",
      "      ID= 13 ('Hello') prob=0.1632\n",
      "      ID= 22 ('Test') prob=0.0805\n",
      "      ID= 34 ('dog') prob=0.0699\n",
      "\n",
      "  Step 3:\n",
      "    Selected: ID=13 ('Hello') prob=0.1808\n",
      "    Top 3 candidates:\n",
      "      ID= 13 ('Hello') prob=0.1808\n",
      "      ID= 22 ('Test') prob=0.0708\n",
      "      ID= 34 ('dog') prob=0.0644\n",
      "\n",
      "  Step 4:\n",
      "    Selected: ID=13 ('Hello') prob=0.1988\n",
      "    Top 3 candidates:\n",
      "      ID= 13 ('Hello') prob=0.1988\n",
      "      ID= 22 ('Test') prob=0.0606\n",
      "      ID= 34 ('dog') prob=0.0569\n",
      "\n",
      "  Step 5:\n",
      "    Selected: ID=13 ('Hello') prob=0.2113\n",
      "    Top 3 candidates:\n",
      "      ID= 13 ('Hello') prob=0.2113\n",
      "      ID= 22 ('Test') prob=0.0540\n",
      "      ID= 34 ('dog') prob=0.0508\n",
      "\n",
      "✓ Decoding completed in 20 steps!\n"
     ]
    }
   ],
   "source": [
    "# Show step-by-step decoding\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Step-by-Step Decoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDecoding steps (first 5):\")\n",
    "for info in step_info[:5]:\n",
    "    token_str = tokenizer.decode([info['selected']])\n",
    "    print(f\"\\n  Step {info['step']}:\")\n",
    "    print(f\"    Selected: ID={info['selected']} ('{token_str}') prob={info['selected_prob']:.4f}\")\n",
    "    print(f\"    Top 3 candidates:\")\n",
    "    for tid, prob in info['top3']:\n",
    "        t_str = tokenizer.decode([tid])\n",
    "        print(f\"      ID={tid:3d} ('{t_str}') prob={prob:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Decoding completed in {len(step_info)} steps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Using the Built-in Generate Method\n",
    "\n",
    "The Transformer class has a built-in generate() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Built-in Generate Method\n",
      "============================================================\n",
      "\n",
      "Input:  'The cat sat on the mat'\n",
      "Output: 'Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello'\n",
      "\n",
      "Generated shape: torch.Size([1, 20])\n",
      "Output IDs: [2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "\n",
      "✓ Built-in generate() works correctly!\n"
     ]
    }
   ],
   "source": [
    "# Use built-in generate method\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        src=src_tensor,\n",
    "        max_len=20,\n",
    "        start_token=tokenizer.bos_id,\n",
    "        end_token=tokenizer.eos_id,\n",
    "    )\n",
    "\n",
    "output_ids = generated[0].tolist()\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Built-in Generate Method\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput:  '{input_text}'\")\n",
    "print(f\"Output: '{output_text}'\")\n",
    "print(f\"\\nGenerated shape: {generated.shape}\")\n",
    "print(f\"Output IDs: {output_ids}\")\n",
    "print(f\"\\n✓ Built-in generate() works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Batch Inference\n",
    "\n",
    "Translate multiple sentences at once for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Batch Input Preparation\n",
      "============================================================\n",
      "\n",
      "Number of inputs: 3\n",
      "\n",
      "  Input 1: 'Hello world'\n",
      "  Token IDs: [2, 13, 60, 3]\n",
      "  Length: 4\n",
      "\n",
      "  Input 2: 'Good morning everyone'\n",
      "  Token IDs: [2, 10, 48, 37, 3]\n",
      "  Length: 5\n",
      "\n",
      "  Input 3: 'The dog runs fast'\n",
      "  Token IDs: [2, 4, 34, 51, 38, 3]\n",
      "  Length: 6\n"
     ]
    }
   ],
   "source": [
    "# Multiple inputs\n",
    "test_inputs = [\n",
    "    \"Hello world\",\n",
    "    \"Good morning everyone\",\n",
    "    \"The dog runs fast\",\n",
    "]\n",
    "\n",
    "# Tokenize all\n",
    "src_batch = []\n",
    "for text in test_inputs:\n",
    "    ids = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "    src_batch.append(ids)\n",
    "\n",
    "# Validate: Show first 3 tokenized inputs\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Batch Input Preparation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNumber of inputs: {len(test_inputs)}\")\n",
    "for i, (text, ids) in enumerate(zip(test_inputs, src_batch)):\n",
    "    print(f\"\\n  Input {i+1}: '{text}'\")\n",
    "    print(f\"  Token IDs: {ids}\")\n",
    "    print(f\"  Length: {len(ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After padding:\n",
      "  Tensor shape: torch.Size([3, 6])\n",
      "  Padded tensor:\n",
      "    [2, 13, 60, 3, 0, 0]\n",
      "    [2, 10, 48, 37, 3, 0]\n",
      "    [2, 4, 34, 51, 38, 3]\n"
     ]
    }
   ],
   "source": [
    "# Pad batch\n",
    "src_padded = pad_sequences(src_batch, padding_value=tokenizer.pad_id)\n",
    "src_tensor = src_padded.to(device)\n",
    "\n",
    "print(\"\\nAfter padding:\")\n",
    "print(f\"  Tensor shape: {src_tensor.shape}\")\n",
    "print(f\"  Padded tensor:\")\n",
    "for i in range(src_tensor.size(0)):\n",
    "    print(f\"    {src_tensor[i].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Batch Translation Results\n",
      "============================================================\n",
      "\n",
      "Generated shape: torch.Size([3, 20])\n",
      "\n",
      "Translations:\n",
      "\n",
      "  Input 1:  'Hello world'\n",
      "  Output 1: 'Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello'\n",
      "  IDs: [2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "\n",
      "  Input 2:  'Good morning everyone'\n",
      "  Output 2: 'Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello'\n",
      "  IDs: [2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "\n",
      "  Input 3:  'The dog runs fast'\n",
      "  Output 3: 'Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello'\n",
      "  IDs: [2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "\n",
      "✓ Batch inference completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Batch generate\n",
    "with torch.no_grad():\n",
    "    batch_generated = model.generate(\n",
    "        src=src_tensor,\n",
    "        max_len=20,\n",
    "        start_token=tokenizer.bos_id,\n",
    "        end_token=tokenizer.eos_id,\n",
    "    )\n",
    "\n",
    "# Decode all outputs\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Batch Translation Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGenerated shape: {batch_generated.shape}\")\n",
    "\n",
    "print(f\"\\nTranslations:\")\n",
    "for i in range(len(test_inputs)):\n",
    "    output_ids = batch_generated[i].tolist()\n",
    "    output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(f\"\\n  Input {i+1}:  '{test_inputs[i]}'\")\n",
    "    print(f\"  Output {i+1}: '{output_text}'\")\n",
    "    print(f\"  IDs: {output_ids}\")\n",
    "\n",
    "print(f\"\\n✓ Batch inference completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Examining Model Internals\n",
    "\n",
    "Let's look at the encoder output and attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Model Internals\n",
      "============================================================\n",
      "\n",
      "Input: 'The cat sat'\n",
      "Token IDs: [2, 4, 31, 52, 3]\n",
      "\n",
      "Embedding output:\n",
      "  Shape: torch.Size([1, 5, 128])\n",
      "  Mean: -0.0078\n",
      "  Std: 0.9818\n",
      "\n",
      "Encoder output (memory):\n",
      "  Shape: torch.Size([1, 5, 128])\n",
      "  Mean: 0.0016\n",
      "  Std: 0.8875\n",
      "\n",
      "First token representation (first 8 dims):\n",
      "  Embedding: [0.5833365321159363, -0.3980603814125061, 0.717571496963501, 1.0357662439346313, -0.06894383579492569, -1.0180706977844238, 0.20098383724689484, 1.4401447772979736]\n",
      "  After encoder: [-0.5205539464950562, -0.1300450563430786, -0.26950663328170776, 1.5562264919281006, 0.8999665379524231, -0.5280025601387024, -0.5555990934371948, -1.1567842960357666]\n"
     ]
    }
   ],
   "source": [
    "# Get encoder output for analysis\n",
    "single_input = \"The cat sat\"\n",
    "src_ids = tokenizer.encode(single_input, add_bos=True, add_eos=True)\n",
    "src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode\n",
    "    memory = model.encode(src_tensor)\n",
    "    \n",
    "    # Get embeddings before encoding\n",
    "    embeddings = model.src_embedding(src_tensor)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Model Internals\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nInput: '{single_input}'\")\n",
    "print(f\"Token IDs: {src_ids}\")\n",
    "\n",
    "print(f\"\\nEmbedding output:\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Mean: {embeddings.mean().item():.4f}\")\n",
    "print(f\"  Std: {embeddings.std().item():.4f}\")\n",
    "\n",
    "print(f\"\\nEncoder output (memory):\")\n",
    "print(f\"  Shape: {memory.shape}\")\n",
    "print(f\"  Mean: {memory.mean().item():.4f}\")\n",
    "print(f\"  Std: {memory.std().item():.4f}\")\n",
    "\n",
    "# Show first few dimensions of first token\n",
    "print(f\"\\nFirst token representation (first 8 dims):\")\n",
    "print(f\"  Embedding: {embeddings[0, 0, :8].tolist()}\")\n",
    "print(f\"  After encoder: {memory[0, 0, :8].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Output Distribution (First Step)\n",
      "============================================================\n",
      "\n",
      "Logits shape: torch.Size([1, 62])\n",
      "Probability distribution:\n",
      "  Min prob: 0.000611\n",
      "  Max prob: 0.175444\n",
      "  Sum: 1.0000 (should be 1.0)\n",
      "\n",
      "Top 5 predictions for first output token:\n",
      "  1. ID= 13 ('Hello') prob=0.1754\n",
      "  2. ID= 34 ('dog') prob=0.0787\n",
      "  3. ID= 22 ('Test') prob=0.0751\n",
      "  4. ID=  3 ('') prob=0.0476\n",
      "  5. ID= 50 ('on') prob=0.0322\n",
      "\n",
      "✓ Model internals examination complete!\n"
     ]
    }
   ],
   "source": [
    "# Examine output logits distribution\n",
    "with torch.no_grad():\n",
    "    tgt_start = torch.tensor([[tokenizer.bos_id]], device=device)\n",
    "    tgt_mask = model._create_tgt_mask(tgt_start)\n",
    "    decoder_output = model.decode(tgt_start, memory, tgt_mask)\n",
    "    logits = model.output_projection(decoder_output[:, -1, :])\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Output Distribution (First Step)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nLogits shape: {logits.shape}\")\n",
    "print(f\"Probability distribution:\")\n",
    "print(f\"  Min prob: {probs.min().item():.6f}\")\n",
    "print(f\"  Max prob: {probs.max().item():.6f}\")\n",
    "print(f\"  Sum: {probs.sum().item():.4f} (should be 1.0)\")\n",
    "\n",
    "# Show top 5 predictions\n",
    "top_probs, top_ids = probs.topk(5, dim=-1)\n",
    "print(f\"\\nTop 5 predictions for first output token:\")\n",
    "for i in range(5):\n",
    "    tid = top_ids[0, i].item()\n",
    "    prob = top_probs[0, i].item()\n",
    "    token = tokenizer.decode([tid])\n",
    "    print(f\"  {i+1}. ID={tid:3d} ('{token}') prob={prob:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Model internals examination complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. **Load a trained model** - Restore from checkpoint\n",
    "2. **Prepare input** - Tokenize text for the model\n",
    "3. **Run greedy decoding** - Generate translations step by step\n",
    "4. **Use built-in generate()** - Simplified inference API\n",
    "5. **Batch inference** - Translate multiple sentences efficiently\n",
    "6. **Examine internals** - Understand model representations\n",
    "\n",
    "Each step was validated to ensure correctness.\n",
    "\n",
    "### Notes for Production:\n",
    "- Use beam search instead of greedy decoding for better quality\n",
    "- Implement length normalization for beam search\n",
    "- Add temperature/top-k/top-p sampling for diversity\n",
    "- Cache encoder outputs for repeated decoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
