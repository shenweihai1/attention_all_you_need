{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Inference Tutorial\n",
    "\n",
    "This notebook shows how to use a trained Transformer model for translation.\n",
    "Each step includes validation to verify correctness.\n",
    "\n",
    "## Overview\n",
    "1. Setup and imports\n",
    "2. Load the trained model\n",
    "3. Prepare input text\n",
    "4. Run inference (greedy decoding)\n",
    "5. Analyze attention weights\n",
    "6. Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory to path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import our Transformer implementation\n",
    "from src import Transformer\n",
    "from src.tokenizer import SimpleTokenizer, pad_sequences\n",
    "from src.attention import create_causal_mask, create_padding_mask\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Trained Model\n",
    "\n",
    "Load the checkpoint saved from training tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if checkpoint exists, if not create a demo model\n",
    "checkpoint_path = '../checkpoints/demo_model.pt'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading saved checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    config = checkpoint['config']\n",
    "else:\n",
    "    print(\"No checkpoint found. Creating fresh model for demonstration...\")\n",
    "    config = {\n",
    "        'vocab_size': 100,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 4,\n",
    "        'n_layers': 2,\n",
    "        'd_ff': 256,\n",
    "        'dropout': 0.1,\n",
    "    }\n",
    "    checkpoint = None\n",
    "\n",
    "# Validate: Show configuration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION: Model Configuration\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild tokenizer with same vocabulary\n",
    "# In practice, save and load tokenizer too\n",
    "src_sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"Hello world\",\n",
    "    \"How are you today\",\n",
    "    \"I love machine learning\",\n",
    "    \"The weather is nice\",\n",
    "    \"Good morning everyone\",\n",
    "    \"This is a test\",\n",
    "    \"The dog runs fast\",\n",
    "]\n",
    "\n",
    "tgt_sentences = [\n",
    "    \"Die Katze saß auf der Matte\",\n",
    "    \"Hallo Welt\",\n",
    "    \"Wie geht es dir heute\",\n",
    "    \"Ich liebe maschinelles Lernen\",\n",
    "    \"Das Wetter ist schön\",\n",
    "    \"Guten Morgen allerseits\",\n",
    "    \"Das ist ein Test\",\n",
    "    \"Der Hund läuft schnell\",\n",
    "]\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(src_sentences + tgt_sentences)\n",
    "\n",
    "# Update vocab_size to match tokenizer\n",
    "config['vocab_size'] = tokenizer.vocab_size\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Tokenizer Rebuilt\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"BOS_ID: {tokenizer.bos_id}\")\n",
    "print(f\"EOS_ID: {tokenizer.eos_id}\")\n",
    "print(f\"PAD_ID: {tokenizer.pad_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Transformer(\n",
    "    src_vocab_size=config['vocab_size'],\n",
    "    tgt_vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_encoder_layers=config['n_layers'],\n",
    "    n_decoder_layers=config['n_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout=config['dropout'],\n",
    "    pad_idx=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "# Load weights if checkpoint exists\n",
    "if checkpoint is not None:\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Model weights loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load weights (vocab size mismatch?): {e}\")\n",
    "        print(\"Using fresh model weights.\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION: Model Ready\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel loaded and set to eval mode\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\n✓ Model is ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Input Text\n",
    "\n",
    "Tokenize the input sentence for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sentence to translate\n",
    "input_text = \"The cat sat on the mat\"\n",
    "\n",
    "# Tokenize\n",
    "src_ids = tokenizer.encode(input_text, add_bos=True, add_eos=True)\n",
    "src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "\n",
    "# Validate input\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Input Preparation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput text: '{input_text}'\")\n",
    "print(f\"\\nToken IDs: {src_ids}\")\n",
    "print(f\"Tensor shape: {src_tensor.shape}\")\n",
    "\n",
    "# Show token-by-token breakdown\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, token_id in enumerate(src_ids):\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    print(f\"  Position {i}: ID={token_id:3d} -> '{token_str}'\")\n",
    "\n",
    "print(f\"\\n✓ Input prepared correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Inference (Greedy Decoding)\n",
    "\n",
    "Generate translation using greedy decoding (selecting most probable token at each step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, tokenizer, max_len=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Greedy decoding: select the most probable token at each step.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        src: Source token tensor (1, src_len)\n",
    "        tokenizer: Tokenizer with bos_id, eos_id\n",
    "        max_len: Maximum output length\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Generated token IDs and step-by-step info\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src = src.to(device)\n",
    "    memory = model.encode(src)\n",
    "    \n",
    "    # Start with BOS token\n",
    "    generated = [tokenizer.bos_id]\n",
    "    step_info = []\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        # Create target tensor\n",
    "        tgt = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "        \n",
    "        # Decode\n",
    "        tgt_mask = model._create_tgt_mask(tgt)\n",
    "        decoder_output = model.decode(tgt, memory, tgt_mask)\n",
    "        \n",
    "        # Get logits for last position\n",
    "        logits = model.output_projection(decoder_output[:, -1, :])\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Greedy: select most probable\n",
    "        next_token = probs.argmax(dim=-1).item()\n",
    "        \n",
    "        # Get top-3 for analysis\n",
    "        top_probs, top_ids = probs.topk(3, dim=-1)\n",
    "        \n",
    "        step_info.append({\n",
    "            'step': step + 1,\n",
    "            'selected': next_token,\n",
    "            'selected_prob': probs[0, next_token].item(),\n",
    "            'top3': [(top_ids[0, i].item(), top_probs[0, i].item()) for i in range(3)]\n",
    "        })\n",
    "        \n",
    "        # Append token\n",
    "        generated.append(next_token)\n",
    "        \n",
    "        # Stop if EOS\n",
    "        if next_token == tokenizer.eos_id:\n",
    "            break\n",
    "    \n",
    "    return generated, step_info\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    generated_ids, step_info = greedy_decode(\n",
    "        model, src_tensor, tokenizer, max_len=20, device=device\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Greedy Decoding Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput:  '{input_text}'\")\n",
    "print(f\"Output: '{output_text}'\")\n",
    "print(f\"\\nGenerated IDs: {generated_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step-by-step decoding\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Step-by-Step Decoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDecoding steps (first 5):\")\n",
    "for info in step_info[:5]:\n",
    "    token_str = tokenizer.decode([info['selected']])\n",
    "    print(f\"\\n  Step {info['step']}:\")\n",
    "    print(f\"    Selected: ID={info['selected']} ('{token_str}') prob={info['selected_prob']:.4f}\")\n",
    "    print(f\"    Top 3 candidates:\")\n",
    "    for tid, prob in info['top3']:\n",
    "        t_str = tokenizer.decode([tid])\n",
    "        print(f\"      ID={tid:3d} ('{t_str}') prob={prob:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Decoding completed in {len(step_info)} steps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Using the Built-in Generate Method\n",
    "\n",
    "The Transformer class has a built-in generate() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use built-in generate method\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        src=src_tensor,\n",
    "        max_len=20,\n",
    "        start_token=tokenizer.bos_id,\n",
    "        end_token=tokenizer.eos_id,\n",
    "    )\n",
    "\n",
    "output_ids = generated[0].tolist()\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Built-in Generate Method\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput:  '{input_text}'\")\n",
    "print(f\"Output: '{output_text}'\")\n",
    "print(f\"\\nGenerated shape: {generated.shape}\")\n",
    "print(f\"Output IDs: {output_ids}\")\n",
    "print(f\"\\n✓ Built-in generate() works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Batch Inference\n",
    "\n",
    "Translate multiple sentences at once for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple inputs\n",
    "test_inputs = [\n",
    "    \"Hello world\",\n",
    "    \"Good morning everyone\",\n",
    "    \"The dog runs fast\",\n",
    "]\n",
    "\n",
    "# Tokenize all\n",
    "src_batch = []\n",
    "for text in test_inputs:\n",
    "    ids = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "    src_batch.append(ids)\n",
    "\n",
    "# Validate: Show first 3 tokenized inputs\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Batch Input Preparation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNumber of inputs: {len(test_inputs)}\")\n",
    "for i, (text, ids) in enumerate(zip(test_inputs, src_batch)):\n",
    "    print(f\"\\n  Input {i+1}: '{text}'\")\n",
    "    print(f\"  Token IDs: {ids}\")\n",
    "    print(f\"  Length: {len(ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad batch\n",
    "src_padded = pad_sequences(src_batch, padding_value=tokenizer.pad_id)\n",
    "src_tensor = src_padded.to(device)\n",
    "\n",
    "print(\"\\nAfter padding:\")\n",
    "print(f\"  Tensor shape: {src_tensor.shape}\")\n",
    "print(f\"  Padded tensor:\")\n",
    "for i in range(src_tensor.size(0)):\n",
    "    print(f\"    {src_tensor[i].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generate\n",
    "with torch.no_grad():\n",
    "    batch_generated = model.generate(\n",
    "        src=src_tensor,\n",
    "        max_len=20,\n",
    "        start_token=tokenizer.bos_id,\n",
    "        end_token=tokenizer.eos_id,\n",
    "    )\n",
    "\n",
    "# Decode all outputs\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Batch Translation Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGenerated shape: {batch_generated.shape}\")\n",
    "\n",
    "print(f\"\\nTranslations:\")\n",
    "for i in range(len(test_inputs)):\n",
    "    output_ids = batch_generated[i].tolist()\n",
    "    output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(f\"\\n  Input {i+1}:  '{test_inputs[i]}'\")\n",
    "    print(f\"  Output {i+1}: '{output_text}'\")\n",
    "    print(f\"  IDs: {output_ids}\")\n",
    "\n",
    "print(f\"\\n✓ Batch inference completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Examining Model Internals\n",
    "\n",
    "Let's look at the encoder output and attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoder output for analysis\n",
    "single_input = \"The cat sat\"\n",
    "src_ids = tokenizer.encode(single_input, add_bos=True, add_eos=True)\n",
    "src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode\n",
    "    memory = model.encode(src_tensor)\n",
    "    \n",
    "    # Get embeddings before encoding\n",
    "    embeddings = model.src_embedding(src_tensor)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Model Internals\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nInput: '{single_input}'\")\n",
    "print(f\"Token IDs: {src_ids}\")\n",
    "\n",
    "print(f\"\\nEmbedding output:\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Mean: {embeddings.mean().item():.4f}\")\n",
    "print(f\"  Std: {embeddings.std().item():.4f}\")\n",
    "\n",
    "print(f\"\\nEncoder output (memory):\")\n",
    "print(f\"  Shape: {memory.shape}\")\n",
    "print(f\"  Mean: {memory.mean().item():.4f}\")\n",
    "print(f\"  Std: {memory.std().item():.4f}\")\n",
    "\n",
    "# Show first few dimensions of first token\n",
    "print(f\"\\nFirst token representation (first 8 dims):\")\n",
    "print(f\"  Embedding: {embeddings[0, 0, :8].tolist()}\")\n",
    "print(f\"  After encoder: {memory[0, 0, :8].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine output logits distribution\n",
    "with torch.no_grad():\n",
    "    tgt_start = torch.tensor([[tokenizer.bos_id]], device=device)\n",
    "    tgt_mask = model._create_tgt_mask(tgt_start)\n",
    "    decoder_output = model.decode(tgt_start, memory, tgt_mask)\n",
    "    logits = model.output_projection(decoder_output[:, -1, :])\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Output Distribution (First Step)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nLogits shape: {logits.shape}\")\n",
    "print(f\"Probability distribution:\")\n",
    "print(f\"  Min prob: {probs.min().item():.6f}\")\n",
    "print(f\"  Max prob: {probs.max().item():.6f}\")\n",
    "print(f\"  Sum: {probs.sum().item():.4f} (should be 1.0)\")\n",
    "\n",
    "# Show top 5 predictions\n",
    "top_probs, top_ids = probs.topk(5, dim=-1)\n",
    "print(f\"\\nTop 5 predictions for first output token:\")\n",
    "for i in range(5):\n",
    "    tid = top_ids[0, i].item()\n",
    "    prob = top_probs[0, i].item()\n",
    "    token = tokenizer.decode([tid])\n",
    "    print(f\"  {i+1}. ID={tid:3d} ('{token}') prob={prob:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Model internals examination complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. **Load a trained model** - Restore from checkpoint\n",
    "2. **Prepare input** - Tokenize text for the model\n",
    "3. **Run greedy decoding** - Generate translations step by step\n",
    "4. **Use built-in generate()** - Simplified inference API\n",
    "5. **Batch inference** - Translate multiple sentences efficiently\n",
    "6. **Examine internals** - Understand model representations\n",
    "\n",
    "Each step was validated to ensure correctness.\n",
    "\n",
    "### Notes for Production:\n",
    "- Use beam search instead of greedy decoding for better quality\n",
    "- Implement length normalization for beam search\n",
    "- Add temperature/top-k/top-p sampling for diversity\n",
    "- Cache encoder outputs for repeated decoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
