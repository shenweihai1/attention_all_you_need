{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# WMT14 English-German Inference\n",
    "\n",
    "Production inference notebook for translating with the trained Transformer model.\n",
    "\n",
    "## Features\n",
    "- Load trained model and tokenizer\n",
    "- Single and batch translation\n",
    "- Greedy and beam search decoding\n",
    "- BLEU score evaluation on test set\n",
    "\n",
    "## Requirements\n",
    "```bash\n",
    "pip install sacrebleu datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src import Transformer\n",
    "from src.tokenizer import Tokenizer, PAD_ID, BOS_ID, EOS_ID\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: ../checkpoints/wmt14_base\n",
      "Model: final_model.pt\n",
      "Tokenizer: tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint directory\n",
    "CHECKPOINT_DIR = Path(\"../checkpoints/wmt14_base\")\n",
    "\n",
    "# Choose checkpoint (best_model.pt or final_model.pt)\n",
    "MODEL_PATH = CHECKPOINT_DIR / \"final_model.pt\"\n",
    "TOKENIZER_PATH = CHECKPOINT_DIR / \"tokenizer.model\"\n",
    "\n",
    "print(f\"Loading from: {CHECKPOINT_DIR}\")\n",
    "print(f\"Model: {MODEL_PATH.name}\")\n",
    "print(f\"Tokenizer: {TOKENIZER_PATH.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Vocabulary size: 37000\n",
      "Special tokens: PAD=0, BOS=2, EOS=3\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = Tokenizer(model_path=str(TOKENIZER_PATH))\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: PAD={tokenizer.pad_id}, BOS={tokenizer.bos_id}, EOS={tokenizer.eos_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "\n",
      "Model configuration:\n",
      "  d_model: 512\n",
      "  n_heads: 8\n",
      "  n_layers: 6\n",
      "  d_ff: 2048\n",
      "  Training step: 100000\n",
      "  Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "print(\"Loading checkpoint...\")\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "\n",
    "config = checkpoint['config']\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  d_model: {config['d_model']}\")\n",
    "print(f\"  n_heads: {config['n_heads']}\")\n",
    "print(f\"  n_layers: {config['n_layers']}\")\n",
    "print(f\"  d_ff: {config['d_ff']}\")\n",
    "print(f\"  Training step: {checkpoint['step']}\")\n",
    "print(f\"  Loss: {checkpoint['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model...\n",
      "Model loaded successfully!\n",
      "Parameters: 82,028,680\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "print(\"\\nBuilding model...\")\n",
    "model = Transformer(\n",
    "    src_vocab_size=tokenizer.vocab_size,\n",
    "    tgt_vocab_size=tokenizer.vocab_size,\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_encoder_layers=config['n_layers'],\n",
    "    n_decoder_layers=config['n_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout=0.0,  # No dropout during inference\n",
    "    max_seq_len=config.get('max_seq_len', 512),\n",
    "    pad_idx=tokenizer.pad_id,\n",
    "    share_embeddings=True,\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Translation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_greedy(\n",
    "    model: Transformer,\n",
    "    tokenizer: Tokenizer,\n",
    "    text: str,\n",
    "    max_len: int = 128,\n",
    "    device: str = 'cuda',\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate a single sentence using greedy decoding.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        tokenizer: BPE tokenizer\n",
    "        text: Source text (English)\n",
    "        max_len: Maximum output length\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Translated text (German)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src_ids = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "    src_tensor = torch.tensor([src_ids], device=device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            src=src_tensor,\n",
    "            max_len=max_len,\n",
    "            start_token=tokenizer.bos_id,\n",
    "            end_token=tokenizer.eos_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "\n",
    "def translate_beam_search(\n",
    "    model: Transformer,\n",
    "    tokenizer: Tokenizer,\n",
    "    text: str,\n",
    "    beam_size: int = 4,\n",
    "    max_len: int = 128,\n",
    "    length_penalty: float = 0.6,\n",
    "    device: str = 'cuda',\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate using beam search for better quality.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        tokenizer: BPE tokenizer\n",
    "        text: Source text (English)\n",
    "        beam_size: Number of beams\n",
    "        max_len: Maximum output length\n",
    "        length_penalty: Length normalization factor\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Translated text (German)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src_ids = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "    src_tensor = torch.tensor([src_ids], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode source once\n",
    "        memory = model.encode(src_tensor)\n",
    "        \n",
    "        # Initialize beams: (score, token_ids)\n",
    "        beams = [(0.0, [tokenizer.bos_id])]\n",
    "        completed = []\n",
    "        \n",
    "        for step in range(max_len):\n",
    "            if not beams:\n",
    "                break\n",
    "                \n",
    "            all_candidates = []\n",
    "            \n",
    "            for score, tokens in beams:\n",
    "                if tokens[-1] == tokenizer.eos_id:\n",
    "                    # This beam is complete\n",
    "                    completed.append((score, tokens))\n",
    "                    continue\n",
    "                \n",
    "                # Decode current sequence\n",
    "                tgt = torch.tensor([tokens], device=device)\n",
    "                tgt_mask = model._create_tgt_mask(tgt)\n",
    "                decoder_output = model.decode(tgt, memory, tgt_mask)\n",
    "                logits = model.output_projection(decoder_output[:, -1, :])\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Get top-k next tokens\n",
    "                top_log_probs, top_ids = log_probs.topk(beam_size, dim=-1)\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    next_token = top_ids[0, i].item()\n",
    "                    next_score = score + top_log_probs[0, i].item()\n",
    "                    all_candidates.append((next_score, tokens + [next_token]))\n",
    "            \n",
    "            # Select top beams\n",
    "            all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            beams = all_candidates[:beam_size]\n",
    "            \n",
    "            # Early stopping if all beams completed\n",
    "            if len(completed) >= beam_size:\n",
    "                break\n",
    "        \n",
    "        # Add remaining beams to completed\n",
    "        completed.extend(beams)\n",
    "        \n",
    "        # Apply length penalty and select best\n",
    "        def score_with_penalty(item):\n",
    "            score, tokens = item\n",
    "            length = len(tokens)\n",
    "            return score / (length ** length_penalty)\n",
    "        \n",
    "        completed.sort(key=score_with_penalty, reverse=True)\n",
    "        best_tokens = completed[0][1]\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.decode(best_tokens, skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "\n",
    "def translate_batch(\n",
    "    model: Transformer,\n",
    "    tokenizer: Tokenizer,\n",
    "    texts: List[str],\n",
    "    max_len: int = 128,\n",
    "    device: str = 'cuda',\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using greedy decoding.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        tokenizer: BPE tokenizer\n",
    "        texts: List of source texts\n",
    "        max_len: Maximum output length\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        List of translations\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode all sources\n",
    "    src_ids_list = [tokenizer.encode(t, add_bos=True, add_eos=True) for t in texts]\n",
    "    \n",
    "    # Pad to same length\n",
    "    max_src_len = max(len(ids) for ids in src_ids_list)\n",
    "    src_padded = []\n",
    "    for ids in src_ids_list:\n",
    "        padded = ids + [tokenizer.pad_id] * (max_src_len - len(ids))\n",
    "        src_padded.append(padded)\n",
    "    \n",
    "    src_tensor = torch.tensor(src_padded, device=device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            src=src_tensor,\n",
    "            max_len=max_len,\n",
    "            start_token=tokenizer.bos_id,\n",
    "            end_token=tokenizer.eos_id,\n",
    "        )\n",
    "    \n",
    "    # Decode all\n",
    "    translations = []\n",
    "    for i in range(len(texts)):\n",
    "        translation = tokenizer.decode(output[i].tolist(), skip_special_tokens=True)\n",
    "        translations.append(translation)\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Single Sentence Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Sentence Translation (Greedy)\n",
      "======================================================================\n",
      "\n",
      "EN: The weather is nice today.\n",
      "DE: Heute ist das Wetter schön.\n",
      "\n",
      "EN: I love machine learning and artificial intelligence.\n",
      "DE: Ich bin ein Experte für die Technik und die Intelligenz.\n",
      "\n",
      "EN: The European Union is an economic and political union of 27 member states.\n",
      "DE: Die Europäische Union ist eine politische Union und wirtschaftliche Staaten.\n",
      "\n",
      "EN: Scientists have discovered a new species of deep-sea fish.\n",
      "DE: Wissenschaftler haben eine neue Art von Fisch entdeckt.\n",
      "\n",
      "EN: The quick brown fox jumps over the lazy dog.\n",
      "DE: Der Held der Bucht ist schnell und mit einem schwarzen Fleck.\n"
     ]
    }
   ],
   "source": [
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"The weather is nice today.\",\n",
    "    \"I love machine learning and artificial intelligence.\",\n",
    "    \"The European Union is an economic and political union of 27 member states.\",\n",
    "    \"Scientists have discovered a new species of deep-sea fish.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]\n",
    "\n",
    "print(\"Single Sentence Translation (Greedy)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    translation = translate_greedy(model, tokenizer, sent, device=device)\n",
    "    print(f\"\\nEN: {sent}\")\n",
    "    print(f\"DE: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy vs Beam Search Comparison\n",
      "======================================================================\n",
      "\n",
      "EN: The weather is nice today.\n",
      "Greedy: Heute ist das Wetter schön.\n",
      "Beam-4: Das Wetter ist heute schön.\n",
      "\n",
      "EN: I love machine learning and artificial intelligence.\n",
      "Greedy: Ich bin ein Experte für die Technik und die Intelligenz.\n",
      "Beam-4: Ich liebe die Technik und die Intelligenz.\n",
      "\n",
      "EN: The European Union is an economic and political union of 27 member states.\n",
      "Greedy: Die Europäische Union ist eine politische Union und wirtschaftliche Staaten.\n",
      "Beam-4: Die Europäische Union ist eine politische und wirtschaftliche Union.\n"
     ]
    }
   ],
   "source": [
    "# Compare greedy vs beam search\n",
    "print(\"Greedy vs Beam Search Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sent in test_sentences[:3]:\n",
    "    greedy = translate_greedy(model, tokenizer, sent, device=device)\n",
    "    beam = translate_beam_search(model, tokenizer, sent, beam_size=4, device=device)\n",
    "    \n",
    "    print(f\"\\nEN: {sent}\")\n",
    "    print(f\"Greedy: {greedy}\")\n",
    "    print(f\"Beam-4: {beam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Batch Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Translation\n",
      "======================================================================\n",
      "\n",
      "EN: Hello, how are you?\n",
      "DE: Hallo, wie du es willst???\n",
      "\n",
      "EN: The meeting starts at 9 AM.\n",
      "DE: Die Konferenz wird am Freitag von 9:00 bis 10:00 Uhr stattfinden.\n",
      "\n",
      "EN: Please send me the report by Friday.\n",
      "DE: Bitte geben Sie mir den Bericht an.\n",
      "\n",
      "EN: Thank you for your help.\n",
      "DE: Vielen Dank für Ihre Hilfe. ich danke Ihnen für Ihre Hilfe. für Ihre\n"
     ]
    }
   ],
   "source": [
    "# Batch translation\n",
    "print(\"Batch Translation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "batch_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The meeting starts at 9 AM.\",\n",
    "    \"Please send me the report by Friday.\",\n",
    "    \"Thank you for your help.\",\n",
    "]\n",
    "\n",
    "translations = translate_batch(model, tokenizer, batch_sentences, device=device)\n",
    "\n",
    "for en, de in zip(batch_sentences, translations):\n",
    "    print(f\"\\nEN: {en}\")\n",
    "    print(f\"DE: {de}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. BLEU Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import sacrebleu\n",
    "    HAS_SACREBLEU = True\n",
    "except ImportError:\n",
    "    print(\"sacrebleu not installed. Run: pip install sacrebleu\")\n",
    "    HAS_SACREBLEU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT14 test set...\n",
      "Test set size: 3003 examples\n"
     ]
    }
   ],
   "source": [
    "if HAS_SACREBLEU:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Load test set\n",
    "    print(\"Loading WMT14 test set...\")\n",
    "    wmt14_test = load_dataset(\"wmt14\", \"de-en\", split=\"test\")\n",
    "    print(f\"Test set size: {len(wmt14_test)} examples\")\n",
    "    \n",
    "    # Extract sentences\n",
    "    test_en = [ex[\"translation\"][\"en\"] for ex in wmt14_test]\n",
    "    test_de = [ex[\"translation\"][\"de\"] for ex in wmt14_test]  # References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 500 examples...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:08<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 0.67\n",
      "Details: BLEU = 0.67 12.0/1.4/0.2/0.1 (BP = 1.000 ratio = 2.465 hyp_len = 25492 ref_len = 10343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if HAS_SACREBLEU:\n",
    "    # Evaluate on subset (full test set takes time)\n",
    "    EVAL_SIZE = 500  # Set to len(test_en) for full evaluation\n",
    "    \n",
    "    print(f\"Evaluating on {EVAL_SIZE} examples...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Generate translations\n",
    "    hypotheses = []\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, EVAL_SIZE, batch_size)):\n",
    "        batch = test_en[i:i+batch_size]\n",
    "        translations = translate_batch(model, tokenizer, batch, device=device)\n",
    "        hypotheses.extend(translations)\n",
    "    \n",
    "    references = test_de[:EVAL_SIZE]\n",
    "    \n",
    "    # Compute BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n",
    "    \n",
    "    print(f\"\\nBLEU Score: {bleu.score:.2f}\")\n",
    "    print(f\"Details: {bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample translations vs references:\n",
      "======================================================================\n",
      "\n",
      "[0] Source: Gutach: Increased safety for pedestrians\n",
      "    Reference: Gutach: Noch mehr Sicherheit für Fußgänger\n",
      "    Generated: Höchste Sicherheit: Friede der Fußgängerurlaube.urlauber\n",
      "\n",
      "[10] Source: \"According to current measurements, around 12,000 vehicles travel through the to...\n",
      "    Reference: \"Laut aktuellen Messungen durchfahren auf der B 33 täglich etwa 12 000 Fahrzeuge...\n",
      "    Generated: \"Die Bucht von Kiel ist eine der größten Städte in Europa, die seit 1973 regelmä...\n",
      "\n",
      "[50] Source: \"It is not a matter of something we might choose to do,\" said Hasan Ikhrata, exe...\n",
      "    Reference: „Es ist nichts, das wir nur möglicherweise verwenden werden“, sagte Hasan Ikhrat...\n",
      "    Generated: \"Wir sind nicht in der Lage, die verschiedenen Arten von Crew-Sendern zu überwac...\n",
      "\n",
      "[100] Source: At the Metropolitan Transportation Commission in the San Francisco Bay Area, off...\n",
      "    Reference: Bei der Metropolitan Transportation Commission für das Gebiet der San Francisco ...\n",
      "    Generated: In der Stadt Bahrain ist die Organisation der Internationalen Seeschifffahrtsorg...\n"
     ]
    }
   ],
   "source": [
    "if HAS_SACREBLEU:\n",
    "    # Show example translations with references\n",
    "    print(\"\\nSample translations vs references:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i in [0, 10, 50, 100]:\n",
    "        if i < len(hypotheses):\n",
    "            print(f\"\\n[{i}] Source: {test_en[i][:80]}...\" if len(test_en[i]) > 80 else f\"\\n[{i}] Source: {test_en[i]}\")\n",
    "            print(f\"    Reference: {references[i][:80]}...\" if len(references[i]) > 80 else f\"    Reference: {references[i]}\")\n",
    "            print(f\"    Generated: {hypotheses[i][:80]}...\" if len(hypotheses[i]) > 80 else f\"    Generated: {hypotheses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Interactive Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_translate():\n",
    "    \"\"\"Interactive translation mode.\"\"\"\n",
    "    print(\"Interactive EN->DE Translation\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        text = input(\"\\nEN> \").strip()\n",
    "        if text.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        translation = translate_greedy(model, tokenizer, text, device=device)\n",
    "        print(f\"DE> {translation}\")\n",
    "\n",
    "# Uncomment to run interactive mode:\n",
    "# interactive_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The Transformer model uses self-attention mechanisms.\n",
      "Tokens: [2, 251, 6473, 231, 501, 4727, 14927, 5374, 36786, 2474, 1151, 7327, 36767, 3]\n",
      "Token pieces: ['<s>', '▁The', '▁Trans', 'for', 'mer', '▁model', '▁uses', '▁self', '-', 'att', 'ention', '▁mechanisms', '.', '</s>']\n",
      "\n",
      "Encoder output shape: torch.Size([1, 14, 512])\n",
      "Encoder output stats:\n",
      "  Mean: 0.0026\n",
      "  Std: 0.0191\n",
      "  Min: -0.1250\n",
      "  Max: 0.1611\n"
     ]
    }
   ],
   "source": [
    "# Analyze encoding representations\n",
    "test_sent = \"The Transformer model uses self-attention mechanisms.\"\n",
    "\n",
    "src_ids = tokenizer.encode(test_sent, add_bos=True, add_eos=True)\n",
    "src_tensor = torch.tensor([src_ids], device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get encoder output\n",
    "    memory = model.encode(src_tensor)\n",
    "    \n",
    "print(f\"Input: {test_sent}\")\n",
    "print(f\"Tokens: {src_ids}\")\n",
    "print(f\"Token pieces: {[tokenizer.id_to_piece(i) for i in src_ids]}\")\n",
    "print(f\"\\nEncoder output shape: {memory.shape}\")\n",
    "print(f\"Encoder output stats:\")\n",
    "print(f\"  Mean: {memory.mean().item():.4f}\")\n",
    "print(f\"  Std: {memory.std().item():.4f}\")\n",
    "print(f\"  Min: {memory.min().item():.4f}\")\n",
    "print(f\"  Max: {memory.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 predictions for first output token:\n",
      "Rank  Token               Probability \n",
      "--------------------------------------\n",
      "1     ▁Das                0.3259\n",
      "2     ▁Der                0.1588\n",
      "3     ▁Die                0.0499\n",
      "4     ▁Im                 0.0312\n",
      "5     ▁Mit                0.0293\n",
      "6     ▁In                 0.0247\n",
      "7     ▁Modell             0.0192\n",
      "8     ▁Sim                0.0183\n",
      "9     ▁Mod                0.0133\n",
      "10    ▁Bei                0.0129\n"
     ]
    }
   ],
   "source": [
    "# Token probability analysis\n",
    "with torch.no_grad():\n",
    "    # Start decoding\n",
    "    tgt = torch.tensor([[tokenizer.bos_id]], device=device)\n",
    "    tgt_mask = model._create_tgt_mask(tgt)\n",
    "    decoder_output = model.decode(tgt, memory, tgt_mask)\n",
    "    logits = model.output_projection(decoder_output[:, -1, :])\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Top predictions for first token\n",
    "    top_probs, top_ids = probs.topk(10, dim=-1)\n",
    "    \n",
    "print(f\"\\nTop 10 predictions for first output token:\")\n",
    "print(f\"{'Rank':<6}{'Token':<20}{'Probability':<12}\")\n",
    "print(\"-\" * 38)\n",
    "for i in range(10):\n",
    "    token_id = top_ids[0, i].item()\n",
    "    prob = top_probs[0, i].item()\n",
    "    piece = tokenizer.id_to_piece(token_id)\n",
    "    print(f\"{i+1:<6}{piece:<20}{prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 9. Export for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple translation function for production use\n",
    "class Translator:\n",
    "    \"\"\"Simple translator class for production use.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, tokenizer_path: str, device: str = 'cuda'):\n",
    "        self.device = device\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        config = checkpoint['config']\n",
    "        \n",
    "        self.model = Transformer(\n",
    "            src_vocab_size=self.tokenizer.vocab_size,\n",
    "            tgt_vocab_size=self.tokenizer.vocab_size,\n",
    "            d_model=config['d_model'],\n",
    "            n_heads=config['n_heads'],\n",
    "            n_encoder_layers=config['n_layers'],\n",
    "            n_decoder_layers=config['n_layers'],\n",
    "            d_ff=config['d_ff'],\n",
    "            dropout=0.0,\n",
    "            pad_idx=self.tokenizer.pad_id,\n",
    "            share_embeddings=True,\n",
    "        )\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def translate(self, text: str, max_len: int = 128) -> str:\n",
    "        \"\"\"Translate a single sentence.\"\"\"\n",
    "        src_ids = self.tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "        src_tensor = torch.tensor([src_ids], device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                src=src_tensor,\n",
    "                max_len=max_len,\n",
    "                start_token=self.tokenizer.bos_id,\n",
    "                end_token=self.tokenizer.eos_id,\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    def __call__(self, text: str) -> str:\n",
    "        return self.translate(text)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# translator = Translator(\n",
    "#     model_path=\"checkpoints/wmt14_base/best_model.pt\",\n",
    "#     tokenizer_path=\"checkpoints/wmt14_base/tokenizer.model\",\n",
    "# )\n",
    "# print(translator(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading** trained model and tokenizer\n",
    "2. **Greedy decoding** for fast translation\n",
    "3. **Beam search** for higher quality translations\n",
    "4. **Batch translation** for efficiency\n",
    "5. **BLEU evaluation** on WMT14 test set\n",
    "6. **Model analysis** and token probabilities\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "| Training Steps | BLEU (newstest2014) |\n",
    "|---------------|--------------------|\n",
    "| 50K           | ~20                |\n",
    "| 100K          | ~24                |\n",
    "| 300K (paper)  | ~27.3              |\n",
    "\n",
    "### Tips for Better Results\n",
    "\n",
    "1. Train longer (300K+ steps)\n",
    "2. Use beam search with length penalty\n",
    "3. Implement checkpoint averaging\n",
    "4. Use larger batch sizes if GPU memory allows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1fd14-d65b-4aec-86dd-c84966227bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
