{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# WMT14 English-German Inference\n",
    "\n",
    "Production inference notebook for translating with the trained Transformer model.\n",
    "\n",
    "## Features\n",
    "- Load trained model and tokenizer\n",
    "- Single and batch translation\n",
    "- Greedy and beam search decoding\n",
    "- BLEU score evaluation on test set\n",
    "\n",
    "## Requirements\n",
    "```bash\n",
    "pip install sacrebleu datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src import Transformer\n",
    "from src.tokenizer import Tokenizer, PAD_ID, BOS_ID, EOS_ID\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint directory\n",
    "CHECKPOINT_DIR = Path(\"../checkpoints/wmt14_base\")\n",
    "\n",
    "# Choose checkpoint (best_model.pt or final_model.pt)\n",
    "MODEL_PATH = CHECKPOINT_DIR / \"best_model.pt\"\n",
    "TOKENIZER_PATH = CHECKPOINT_DIR / \"tokenizer.model\"\n",
    "\n",
    "print(f\"Loading from: {CHECKPOINT_DIR}\")\n",
    "print(f\"Model: {MODEL_PATH.name}\")\n",
    "print(f\"Tokenizer: {TOKENIZER_PATH.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = Tokenizer(model_path=str(TOKENIZER_PATH))\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: PAD={tokenizer.pad_id}, BOS={tokenizer.bos_id}, EOS={tokenizer.eos_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "print(\"Loading checkpoint...\")\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "\n",
    "config = checkpoint['config']\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  d_model: {config['d_model']}\")\n",
    "print(f\"  n_heads: {config['n_heads']}\")\n",
    "print(f\"  n_layers: {config['n_layers']}\")\n",
    "print(f\"  d_ff: {config['d_ff']}\")\n",
    "print(f\"  Training step: {checkpoint['step']}\")\n",
    "print(f\"  Loss: {checkpoint['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "print(\"\\nBuilding model...\")\n",
    "model = Transformer(\n",
    "    src_vocab_size=tokenizer.vocab_size,\n",
    "    tgt_vocab_size=tokenizer.vocab_size,\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_encoder_layers=config['n_layers'],\n",
    "    n_decoder_layers=config['n_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout=0.0,  # No dropout during inference\n",
    "    max_seq_len=config.get('max_seq_len', 512),\n",
    "    pad_idx=tokenizer.pad_id,\n",
    "    share_embeddings=True,\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Translation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_greedy(\n",
    "    model: Transformer,\n",
    "    tokenizer: Tokenizer,\n",
    "    text: str,\n",
    "    max_len: int = 128,\n",
    "    device: str = 'cuda',\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate a single sentence using greedy decoding.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        tokenizer: BPE tokenizer\n",
    "        text: Source text (English)\n",
    "        max_len: Maximum output length\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Translated text (German)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src_ids = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "    src_tensor = torch.tensor([src_ids], device=device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            src=src_tensor,\n",
    "            max_len=max_len,\n",
    "            start_token=tokenizer.bos_id,\n",
    "            end_token=tokenizer.eos_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "\n",
    "def translate_beam_search(\n",
    "    model: Transformer,\n",
    "    tokenizer: Tokenizer,\n",
    "    text: str,\n",
    "    beam_size: int = 4,\n",
    "    max_len: int = 128,\n",
    "    length_penalty: float = 0.6,\n",
    "    device: str = 'cuda',\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate using beam search for better quality.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        tokenizer: BPE tokenizer\n",
    "        text: Source text (English)\n",
    "        beam_size: Number of beams\n",
    "        max_len: Maximum output length\n",
    "        length_penalty: Length normalization factor\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Translated text (German)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src_ids = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "    src_tensor = torch.tensor([src_ids], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode source once\n",
    "        memory = model.encode(src_tensor)\n",
    "        \n",
    "        # Initialize beams: (score, token_ids)\n",
    "        beams = [(0.0, [tokenizer.bos_id])]\n",
    "        completed = []\n",
    "        \n",
    "        for step in range(max_len):\n",
    "            if not beams:\n",
    "                break\n",
    "                \n",
    "            all_candidates = []\n",
    "            \n",
    "            for score, tokens in beams:\n",
    "                if tokens[-1] == tokenizer.eos_id:\n",
    "                    # This beam is complete\n",
    "                    completed.append((score, tokens))\n",
    "                    continue\n",
    "                \n",
    "                # Decode current sequence\n",
    "                tgt = torch.tensor([tokens], device=device)\n",
    "                tgt_mask = model._create_tgt_mask(tgt)\n",
    "                decoder_output = model.decode(tgt, memory, tgt_mask)\n",
    "                logits = model.output_projection(decoder_output[:, -1, :])\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Get top-k next tokens\n",
    "                top_log_probs, top_ids = log_probs.topk(beam_size, dim=-1)\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    next_token = top_ids[0, i].item()\n",
    "                    next_score = score + top_log_probs[0, i].item()\n",
    "                    all_candidates.append((next_score, tokens + [next_token]))\n",
    "            \n",
    "            # Select top beams\n",
    "            all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            beams = all_candidates[:beam_size]\n",
    "            \n",
    "            # Early stopping if all beams completed\n",
    "            if len(completed) >= beam_size:\n",
    "                break\n",
    "        \n",
    "        # Add remaining beams to completed\n",
    "        completed.extend(beams)\n",
    "        \n",
    "        # Apply length penalty and select best\n",
    "        def score_with_penalty(item):\n",
    "            score, tokens = item\n",
    "            length = len(tokens)\n",
    "            return score / (length ** length_penalty)\n",
    "        \n",
    "        completed.sort(key=score_with_penalty, reverse=True)\n",
    "        best_tokens = completed[0][1]\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.decode(best_tokens, skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "\n",
    "def translate_batch(\n",
    "    model: Transformer,\n",
    "    tokenizer: Tokenizer,\n",
    "    texts: List[str],\n",
    "    max_len: int = 128,\n",
    "    device: str = 'cuda',\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using greedy decoding.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        tokenizer: BPE tokenizer\n",
    "        texts: List of source texts\n",
    "        max_len: Maximum output length\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        List of translations\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode all sources\n",
    "    src_ids_list = [tokenizer.encode(t, add_bos=True, add_eos=True) for t in texts]\n",
    "    \n",
    "    # Pad to same length\n",
    "    max_src_len = max(len(ids) for ids in src_ids_list)\n",
    "    src_padded = []\n",
    "    for ids in src_ids_list:\n",
    "        padded = ids + [tokenizer.pad_id] * (max_src_len - len(ids))\n",
    "        src_padded.append(padded)\n",
    "    \n",
    "    src_tensor = torch.tensor(src_padded, device=device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            src=src_tensor,\n",
    "            max_len=max_len,\n",
    "            start_token=tokenizer.bos_id,\n",
    "            end_token=tokenizer.eos_id,\n",
    "        )\n",
    "    \n",
    "    # Decode all\n",
    "    translations = []\n",
    "    for i in range(len(texts)):\n",
    "        translation = tokenizer.decode(output[i].tolist(), skip_special_tokens=True)\n",
    "        translations.append(translation)\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Single Sentence Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"The weather is nice today.\",\n",
    "    \"I love machine learning and artificial intelligence.\",\n",
    "    \"The European Union is an economic and political union of 27 member states.\",\n",
    "    \"Scientists have discovered a new species of deep-sea fish.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]\n",
    "\n",
    "print(\"Single Sentence Translation (Greedy)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    translation = translate_greedy(model, tokenizer, sent, device=device)\n",
    "    print(f\"\\nEN: {sent}\")\n",
    "    print(f\"DE: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare greedy vs beam search\n",
    "print(\"Greedy vs Beam Search Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sent in test_sentences[:3]:\n",
    "    greedy = translate_greedy(model, tokenizer, sent, device=device)\n",
    "    beam = translate_beam_search(model, tokenizer, sent, beam_size=4, device=device)\n",
    "    \n",
    "    print(f\"\\nEN: {sent}\")\n",
    "    print(f\"Greedy: {greedy}\")\n",
    "    print(f\"Beam-4: {beam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Batch Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch translation\n",
    "print(\"Batch Translation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "batch_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The meeting starts at 9 AM.\",\n",
    "    \"Please send me the report by Friday.\",\n",
    "    \"Thank you for your help.\",\n",
    "]\n",
    "\n",
    "translations = translate_batch(model, tokenizer, batch_sentences, device=device)\n",
    "\n",
    "for en, de in zip(batch_sentences, translations):\n",
    "    print(f\"\\nEN: {en}\")\n",
    "    print(f\"DE: {de}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. BLEU Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import sacrebleu\n",
    "    HAS_SACREBLEU = True\n",
    "except ImportError:\n",
    "    print(\"sacrebleu not installed. Run: pip install sacrebleu\")\n",
    "    HAS_SACREBLEU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SACREBLEU:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Load test set\n",
    "    print(\"Loading WMT14 test set...\")\n",
    "    wmt14_test = load_dataset(\"wmt14\", \"de-en\", split=\"test\")\n",
    "    print(f\"Test set size: {len(wmt14_test)} examples\")\n",
    "    \n",
    "    # Extract sentences\n",
    "    test_en = [ex[\"translation\"][\"en\"] for ex in wmt14_test]\n",
    "    test_de = [ex[\"translation\"][\"de\"] for ex in wmt14_test]  # References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SACREBLEU:\n",
    "    # Evaluate on subset (full test set takes time)\n",
    "    EVAL_SIZE = 500  # Set to len(test_en) for full evaluation\n",
    "    \n",
    "    print(f\"Evaluating on {EVAL_SIZE} examples...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Generate translations\n",
    "    hypotheses = []\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, EVAL_SIZE, batch_size)):\n",
    "        batch = test_en[i:i+batch_size]\n",
    "        translations = translate_batch(model, tokenizer, batch, device=device)\n",
    "        hypotheses.extend(translations)\n",
    "    \n",
    "    references = test_de[:EVAL_SIZE]\n",
    "    \n",
    "    # Compute BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n",
    "    \n",
    "    print(f\"\\nBLEU Score: {bleu.score:.2f}\")\n",
    "    print(f\"Details: {bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SACREBLEU:\n",
    "    # Show example translations with references\n",
    "    print(\"\\nSample translations vs references:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i in [0, 10, 50, 100]:\n",
    "        if i < len(hypotheses):\n",
    "            print(f\"\\n[{i}] Source: {test_en[i][:80]}...\" if len(test_en[i]) > 80 else f\"\\n[{i}] Source: {test_en[i]}\")\n",
    "            print(f\"    Reference: {references[i][:80]}...\" if len(references[i]) > 80 else f\"    Reference: {references[i]}\")\n",
    "            print(f\"    Generated: {hypotheses[i][:80]}...\" if len(hypotheses[i]) > 80 else f\"    Generated: {hypotheses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Interactive Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_translate():\n",
    "    \"\"\"Interactive translation mode.\"\"\"\n",
    "    print(\"Interactive EN->DE Translation\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        text = input(\"\\nEN> \").strip()\n",
    "        if text.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        translation = translate_greedy(model, tokenizer, text, device=device)\n",
    "        print(f\"DE> {translation}\")\n",
    "\n",
    "# Uncomment to run interactive mode:\n",
    "# interactive_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze encoding representations\n",
    "test_sent = \"The Transformer model uses self-attention mechanisms.\"\n",
    "\n",
    "src_ids = tokenizer.encode(test_sent, add_bos=True, add_eos=True)\n",
    "src_tensor = torch.tensor([src_ids], device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get encoder output\n",
    "    memory = model.encode(src_tensor)\n",
    "    \n",
    "print(f\"Input: {test_sent}\")\n",
    "print(f\"Tokens: {src_ids}\")\n",
    "print(f\"Token pieces: {[tokenizer.id_to_piece(i) for i in src_ids]}\")\n",
    "print(f\"\\nEncoder output shape: {memory.shape}\")\n",
    "print(f\"Encoder output stats:\")\n",
    "print(f\"  Mean: {memory.mean().item():.4f}\")\n",
    "print(f\"  Std: {memory.std().item():.4f}\")\n",
    "print(f\"  Min: {memory.min().item():.4f}\")\n",
    "print(f\"  Max: {memory.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token probability analysis\n",
    "with torch.no_grad():\n",
    "    # Start decoding\n",
    "    tgt = torch.tensor([[tokenizer.bos_id]], device=device)\n",
    "    tgt_mask = model._create_tgt_mask(tgt)\n",
    "    decoder_output = model.decode(tgt, memory, tgt_mask)\n",
    "    logits = model.output_projection(decoder_output[:, -1, :])\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Top predictions for first token\n",
    "    top_probs, top_ids = probs.topk(10, dim=-1)\n",
    "    \n",
    "print(f\"\\nTop 10 predictions for first output token:\")\n",
    "print(f\"{'Rank':<6}{'Token':<20}{'Probability':<12}\")\n",
    "print(\"-\" * 38)\n",
    "for i in range(10):\n",
    "    token_id = top_ids[0, i].item()\n",
    "    prob = top_probs[0, i].item()\n",
    "    piece = tokenizer.id_to_piece(token_id)\n",
    "    print(f\"{i+1:<6}{piece:<20}{prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 9. Export for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple translation function for production use\n",
    "class Translator:\n",
    "    \"\"\"Simple translator class for production use.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, tokenizer_path: str, device: str = 'cuda'):\n",
    "        self.device = device\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        config = checkpoint['config']\n",
    "        \n",
    "        self.model = Transformer(\n",
    "            src_vocab_size=self.tokenizer.vocab_size,\n",
    "            tgt_vocab_size=self.tokenizer.vocab_size,\n",
    "            d_model=config['d_model'],\n",
    "            n_heads=config['n_heads'],\n",
    "            n_encoder_layers=config['n_layers'],\n",
    "            n_decoder_layers=config['n_layers'],\n",
    "            d_ff=config['d_ff'],\n",
    "            dropout=0.0,\n",
    "            pad_idx=self.tokenizer.pad_id,\n",
    "            share_embeddings=True,\n",
    "        )\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def translate(self, text: str, max_len: int = 128) -> str:\n",
    "        \"\"\"Translate a single sentence.\"\"\"\n",
    "        src_ids = self.tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "        src_tensor = torch.tensor([src_ids], device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                src=src_tensor,\n",
    "                max_len=max_len,\n",
    "                start_token=self.tokenizer.bos_id,\n",
    "                end_token=self.tokenizer.eos_id,\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    def __call__(self, text: str) -> str:\n",
    "        return self.translate(text)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# translator = Translator(\n",
    "#     model_path=\"checkpoints/wmt14_base/best_model.pt\",\n",
    "#     tokenizer_path=\"checkpoints/wmt14_base/tokenizer.model\",\n",
    "# )\n",
    "# print(translator(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading** trained model and tokenizer\n",
    "2. **Greedy decoding** for fast translation\n",
    "3. **Beam search** for higher quality translations\n",
    "4. **Batch translation** for efficiency\n",
    "5. **BLEU evaluation** on WMT14 test set\n",
    "6. **Model analysis** and token probabilities\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "| Training Steps | BLEU (newstest2014) |\n",
    "|---------------|--------------------|\n",
    "| 50K           | ~20                |\n",
    "| 100K          | ~24                |\n",
    "| 300K (paper)  | ~27.3              |\n",
    "\n",
    "### Tips for Better Results\n",
    "\n",
    "1. Train longer (300K+ steps)\n",
    "2. Use beam search with length penalty\n",
    "3. Implement checkpoint averaging\n",
    "4. Use larger batch sizes if GPU memory allows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
