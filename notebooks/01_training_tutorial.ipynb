{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training Tutorial\n",
    "\n",
    "This notebook walks you through training a Transformer model step by step.\n",
    "Each step includes validation to verify correctness.\n",
    "\n",
    "## Overview\n",
    "1. Setup and imports\n",
    "2. Prepare sample data\n",
    "3. Create tokenizer and vocabulary\n",
    "4. Create dataset and dataloader\n",
    "5. Build the Transformer model\n",
    "6. Setup optimizer, scheduler, and loss function\n",
    "7. Training loop\n",
    "8. Save checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "First, let's import all necessary modules and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory to path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import our Transformer implementation\n",
    "from src import Transformer\n",
    "from src.tokenizer import SimpleTokenizer, pad_sequences, PAD_ID, BOS_ID, EOS_ID\n",
    "from src.data import TranslationDataset, TranslationCollator, create_translation_dataloader\n",
    "from src.scheduler import TransformerScheduler, get_lr_at_step\n",
    "from src.label_smoothing import LabelSmoothingLoss\n",
    "from src.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Sample Data\n",
    "\n",
    "We'll use a small English-to-German translation dataset for demonstration.\n",
    "In production, you would use WMT14 or similar datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: First 3 sentence pairs\n",
      "============================================================\n",
      "\n",
      "Pair 1:\n",
      "  Source (EN): The cat sat on the mat\n",
      "  Target (DE): Die Katze saß auf der Matte\n",
      "\n",
      "Pair 2:\n",
      "  Source (EN): Hello world\n",
      "  Target (DE): Hallo Welt\n",
      "\n",
      "Pair 3:\n",
      "  Source (EN): How are you today\n",
      "  Target (DE): Wie geht es dir heute\n",
      "\n",
      "✓ Total sentence pairs: 16\n"
     ]
    }
   ],
   "source": [
    "# Sample parallel sentences (English -> German)\n",
    "src_sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"Hello world\",\n",
    "    \"How are you today\",\n",
    "    \"I love machine learning\",\n",
    "    \"The weather is nice\",\n",
    "    \"Good morning everyone\",\n",
    "    \"This is a test\",\n",
    "    \"The dog runs fast\",\n",
    "    \"She reads a book\",\n",
    "    \"We learn together\",\n",
    "    \"The sun is shining\",\n",
    "    \"Birds fly in the sky\",\n",
    "    \"I drink coffee\",\n",
    "    \"He plays guitar\",\n",
    "    \"They study hard\",\n",
    "    \"The flower is beautiful\",\n",
    "]\n",
    "\n",
    "tgt_sentences = [\n",
    "    \"Die Katze saß auf der Matte\",\n",
    "    \"Hallo Welt\",\n",
    "    \"Wie geht es dir heute\",\n",
    "    \"Ich liebe maschinelles Lernen\",\n",
    "    \"Das Wetter ist schön\",\n",
    "    \"Guten Morgen allerseits\",\n",
    "    \"Das ist ein Test\",\n",
    "    \"Der Hund läuft schnell\",\n",
    "    \"Sie liest ein Buch\",\n",
    "    \"Wir lernen zusammen\",\n",
    "    \"Die Sonne scheint\",\n",
    "    \"Vögel fliegen am Himmel\",\n",
    "    \"Ich trinke Kaffee\",\n",
    "    \"Er spielt Gitarre\",\n",
    "    \"Sie lernen fleißig\",\n",
    "    \"Die Blume ist wunderschön\",\n",
    "]\n",
    "\n",
    "# Validate: Show first 3 lines\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: First 3 sentence pairs\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(3):\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"  Source (EN): {src_sentences[i]}\")\n",
    "    print(f\"  Target (DE): {tgt_sentences[i]}\")\n",
    "\n",
    "print(f\"\\n✓ Total sentence pairs: {len(src_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Tokenizer and Build Vocabulary\n",
    "\n",
    "We use a SimpleTokenizer for this demo. In production, use BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Tokenizer\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 104\n",
      "\n",
      "Special tokens:\n",
      "  PAD_ID = 0\n",
      "  UNK_ID = 1\n",
      "  BOS_ID = 2\n",
      "  EOS_ID = 3\n",
      "\n",
      "Encoding first 3 sentences:\n",
      "\n",
      "  Sentence 1: 'The cat sat on the mat'\n",
      "  Token IDs:   [2, 4, 52, 85, 81, 15, 78, 3]\n",
      "  Decoded:     'The cat sat on the mat'\n",
      "\n",
      "  Sentence 2: 'Hello world'\n",
      "  Token IDs:   [2, 26, 100, 3]\n",
      "  Decoded:     'Hello world'\n",
      "\n",
      "  Sentence 3: 'How are you today'\n",
      "  Token IDs:   [2, 28, 48, 102, 96, 3]\n",
      "  Decoded:     'How are you today'\n",
      "\n",
      "✓ Tokenizer is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer and build vocabulary from all sentences\n",
    "tokenizer = SimpleTokenizer()\n",
    "all_sentences = src_sentences + tgt_sentences\n",
    "tokenizer.build_vocab(all_sentences)\n",
    "\n",
    "# Validate tokenizer\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Tokenizer\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"\\nSpecial tokens:\")\n",
    "print(f\"  PAD_ID = {tokenizer.pad_id}\")\n",
    "print(f\"  UNK_ID = {tokenizer.unk_id}\")\n",
    "print(f\"  BOS_ID = {tokenizer.bos_id}\")\n",
    "print(f\"  EOS_ID = {tokenizer.eos_id}\")\n",
    "\n",
    "# Test encoding/decoding on first 3 sentences\n",
    "print(f\"\\nEncoding first 3 sentences:\")\n",
    "for i in range(3):\n",
    "    tokens = tokenizer.encode(src_sentences[i], add_bos=True, add_eos=True)\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"\\n  Sentence {i+1}: '{src_sentences[i]}'\")\n",
    "    print(f\"  Token IDs:   {tokens}\")\n",
    "    print(f\"  Decoded:     '{decoded}'\")\n",
    "\n",
    "print(f\"\\n✓ Tokenizer is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Dataset and DataLoader\n",
    "\n",
    "Wrap our data in PyTorch Dataset and DataLoader for efficient batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: DataLoader\n",
      "============================================================\n",
      "\n",
      "Dataset size: 16\n",
      "Batch size: 4\n",
      "Number of batches: 4\n",
      "\n",
      "First batch contents:\n",
      "  src shape: torch.Size([4, 7])\n",
      "  tgt shape: torch.Size([4, 7])\n",
      "  src_mask shape: torch.Size([4, 7])\n",
      "\n",
      "First 3 samples in batch:\n",
      "\n",
      "  Sample 1:\n",
      "    src IDs: [2, 28, 48, 102, 96, 3, 0]\n",
      "    tgt IDs: [2, 44, 65, 58, 55, 68, 3]\n",
      "    src decoded: 'How are you today'\n",
      "    tgt decoded: 'Wie geht es dir heute'\n",
      "\n",
      "  Sample 2:\n",
      "    src IDs: [2, 16, 64, 69, 15, 91, 3]\n",
      "    tgt IDs: [2, 40, 62, 47, 27, 3, 0]\n",
      "    src decoded: 'Birds fly in the sky'\n",
      "    tgt decoded: 'Vögel fliegen am Himmel'\n",
      "\n",
      "  Sample 3:\n",
      "    src IDs: [2, 38, 93, 67, 3, 0, 0]\n",
      "    tgt IDs: [2, 11, 14, 61, 3, 0, 0]\n",
      "    src decoded: 'They study hard'\n",
      "    tgt decoded: 'Sie lernen fleißig'\n",
      "\n",
      "✓ DataLoader is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = TranslationDataset(\n",
    "    src_data=src_sentences,\n",
    "    tgt_data=tgt_sentences,\n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    add_bos=True,\n",
    "    add_eos=True,\n",
    ")\n",
    "\n",
    "# Create dataloader with collator for padding\n",
    "collator = TranslationCollator(pad_id=tokenizer.pad_id)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "# Validate: Show first batch\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: DataLoader\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Batch size: 4\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# Get one batch and inspect\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nFirst batch contents:\")\n",
    "print(f\"  src shape: {sample_batch['src'].shape}\")\n",
    "print(f\"  tgt shape: {sample_batch['tgt'].shape}\")\n",
    "print(f\"  src_mask shape: {sample_batch['src_mask'].shape}\")\n",
    "\n",
    "# Show first 3 samples from batch\n",
    "print(f\"\\nFirst 3 samples in batch:\")\n",
    "for i in range(min(3, sample_batch['src'].size(0))):\n",
    "    src_ids = sample_batch['src'][i].tolist()\n",
    "    tgt_ids = sample_batch['tgt'][i].tolist()\n",
    "    print(f\"\\n  Sample {i+1}:\")\n",
    "    print(f\"    src IDs: {src_ids}\")\n",
    "    print(f\"    tgt IDs: {tgt_ids}\")\n",
    "    print(f\"    src decoded: '{tokenizer.decode(src_ids)}'\")\n",
    "    print(f\"    tgt decoded: '{tokenizer.decode(tgt_ids)}'\")\n",
    "\n",
    "print(f\"\\n✓ DataLoader is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Transformer Model\n",
    "\n",
    "Create the Transformer model with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Model Architecture\n",
      "============================================================\n",
      "\n",
      "Model hyperparameters:\n",
      "  vocab_size: 104\n",
      "  d_model: 128\n",
      "  n_heads: 4\n",
      "  n_layers: 2\n",
      "  d_ff: 256\n",
      "  dropout: 0.1\n",
      "\n",
      "Parameter count:\n",
      "  Total: 700,008\n",
      "  Trainable: 700,008\n",
      "\n",
      "Testing forward pass...\n",
      "  Input src shape: torch.Size([4, 7])\n",
      "  Input tgt shape: torch.Size([4, 6])\n",
      "  Output logits shape: torch.Size([4, 6, 104])\n",
      "  Expected shape: (batch, tgt_len-1, vocab_size) = (4, 6, 104)\n",
      "\n",
      "✓ Model is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters (smaller for demo)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 128      # Model dimension (paper uses 512)\n",
    "n_heads = 4        # Attention heads (paper uses 8)\n",
    "n_layers = 2       # Encoder/decoder layers (paper uses 6)\n",
    "d_ff = 256         # FFN dimension (paper uses 2048)\n",
    "dropout = 0.1\n",
    "max_seq_len = 100\n",
    "\n",
    "# Create model\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_encoder_layers=n_layers,\n",
    "    n_decoder_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout,\n",
    "    max_seq_len=max_seq_len,\n",
    "    pad_idx=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Validate model\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Model Architecture\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel hyperparameters:\")\n",
    "print(f\"  vocab_size: {vocab_size}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  n_layers: {n_layers}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"  dropout: {dropout}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\nTesting forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src = sample_batch['src'].to(device)\n",
    "    tgt = sample_batch['tgt'].to(device)\n",
    "    \n",
    "    # Use tgt[:-1] as input (teacher forcing)\n",
    "    tgt_input = tgt[:, :-1]\n",
    "    logits = model(src, tgt_input)\n",
    "    \n",
    "    print(f\"  Input src shape: {src.shape}\")\n",
    "    print(f\"  Input tgt shape: {tgt_input.shape}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    print(f\"  Expected shape: (batch, tgt_len-1, vocab_size) = ({src.size(0)}, {tgt_input.size(1)}, {vocab_size})\")\n",
    "\n",
    "print(f\"\\n✓ Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup Optimizer, Scheduler, and Loss Function\n",
    "\n",
    "Configure training components as described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Training Components\n",
      "============================================================\n",
      "\n",
      "Optimizer: Adam\n",
      "  betas: (0.9, 0.98)\n",
      "  eps: 1e-9\n",
      "\n",
      "Scheduler: TransformerScheduler\n",
      "  d_model: 128\n",
      "  warmup_steps: 100\n",
      "\n",
      "Learning rate schedule (first 5 steps):\n",
      "  Step    1: lr = 0.000088\n",
      "  Step   25: lr = 0.002210\n",
      "  Step   50: lr = 0.004419\n",
      "  Step  100: lr = 0.008839\n",
      "  Step  200: lr = 0.006250\n",
      "\n",
      "Loss function: LabelSmoothingLoss\n",
      "  smoothing: 0.1\n",
      "  padding_idx: 0\n",
      "\n",
      "Test loss computation:\n",
      "  Loss value: 5.1321\n",
      "\n",
      "✓ Training components are configured correctly!\n"
     ]
    }
   ],
   "source": [
    "# Optimizer (Adam with paper's beta values)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1.0,  # Will be overridden by scheduler\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (warmup + inverse sqrt decay)\n",
    "scheduler = TransformerScheduler(\n",
    "    optimizer,\n",
    "    d_model=d_model,\n",
    "    warmup_steps=100,  # Smaller for demo (paper uses 4000)\n",
    ")\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = LabelSmoothingLoss(\n",
    "    smoothing=0.1,\n",
    "    padding_idx=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "# Validate training components\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Training Components\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"  betas: (0.9, 0.98)\")\n",
    "print(f\"  eps: 1e-9\")\n",
    "\n",
    "print(f\"\\nScheduler: TransformerScheduler\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  warmup_steps: 100\")\n",
    "\n",
    "# Show learning rate at different steps\n",
    "print(f\"\\nLearning rate schedule (first 5 steps):\")\n",
    "for step in [1, 25, 50, 100, 200]:\n",
    "    lr = get_lr_at_step(step, d_model=d_model, warmup_steps=100)\n",
    "    print(f\"  Step {step:4d}: lr = {lr:.6f}\")\n",
    "\n",
    "print(f\"\\nLoss function: LabelSmoothingLoss\")\n",
    "print(f\"  smoothing: 0.1\")\n",
    "print(f\"  padding_idx: {tokenizer.pad_id}\")\n",
    "\n",
    "# Test loss computation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src = sample_batch['src'].to(device)\n",
    "    tgt = sample_batch['tgt'].to(device)\n",
    "    tgt_input = tgt[:, :-1]\n",
    "    tgt_output = tgt[:, 1:]\n",
    "    \n",
    "    logits = model(src, tgt_input)\n",
    "    \n",
    "    # Flatten for loss\n",
    "    loss = criterion(\n",
    "        logits.contiguous().view(-1, vocab_size),\n",
    "        tgt_output.contiguous().view(-1)\n",
    "    )\n",
    "    print(f\"\\nTest loss computation:\")\n",
    "    print(f\"  Loss value: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Training components are configured correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training Loop\n",
    "\n",
    "Run the training loop and observe loss decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "\n",
      "Starting training for 20 epochs...\n",
      "Batches per epoch: 4\n",
      "Epoch   1/20 | Loss: 4.9465 | LR: 0.000442\n",
      "Epoch   2/20 | Loss: 4.4643 | LR: 0.000795\n",
      "Epoch   3/20 | Loss: 4.1146 | LR: 0.001149\n",
      "Epoch   4/20 | Loss: 3.7766 | LR: 0.001503\n",
      "Epoch   5/20 | Loss: 3.2914 | LR: 0.001856\n",
      "Epoch   6/20 | Loss: 2.7534 | LR: 0.002210\n",
      "Epoch   7/20 | Loss: 2.2702 | LR: 0.002563\n",
      "Epoch   8/20 | Loss: 1.9482 | LR: 0.002917\n",
      "Epoch   9/20 | Loss: 1.6900 | LR: 0.003270\n",
      "Epoch  10/20 | Loss: 1.8712 | LR: 0.003624\n",
      "Epoch  11/20 | Loss: 1.7007 | LR: 0.003977\n",
      "Epoch  12/20 | Loss: 1.5249 | LR: 0.004331\n",
      "Epoch  13/20 | Loss: 1.3764 | LR: 0.004685\n",
      "Epoch  14/20 | Loss: 1.3637 | LR: 0.005038\n",
      "Epoch  15/20 | Loss: 1.2947 | LR: 0.005392\n",
      "Epoch  16/20 | Loss: 1.6257 | LR: 0.005745\n",
      "Epoch  17/20 | Loss: 1.5812 | LR: 0.006099\n",
      "Epoch  18/20 | Loss: 1.6971 | LR: 0.006452\n",
      "Epoch  19/20 | Loss: 1.6568 | LR: 0.006806\n",
      "Epoch  20/20 | Loss: 1.7142 | LR: 0.007159\n",
      "\n",
      "✓ Training completed!\n",
      "  Final loss: 1.9447\n",
      "  Total steps: 80\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "log_interval = 5  # Log every N batches\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "global_step = 0\n",
    "training_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        \n",
    "        # Teacher forcing: input is tgt[:-1], target is tgt[1:]\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(src, tgt_input)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(\n",
    "            logits.contiguous().view(-1, vocab_size),\n",
    "            tgt_output.contiguous().view(-1)\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        global_step += 1\n",
    "        training_losses.append(loss.item())\n",
    "    \n",
    "    # Print epoch summary\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "print(f\"\\n✓ Training completed!\")\n",
    "print(f\"  Final loss: {training_losses[-1]:.4f}\")\n",
    "print(f\"  Total steps: {global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Training Progress\n",
      "============================================================\n",
      "\n",
      "Loss at different points:\n",
      "  Start (step 1):     4.9212\n",
      "  Middle (step 40):   1.5346\n",
      "  End (step 80):    1.9447\n",
      "\n",
      "Loss decreased during training: ✓ Yes\n",
      "\n",
      "Loss curve (every 10 steps):\n",
      "  Step   1: ████████████████████████████ 4.9212\n",
      "  Step   9: ██████████████████████ 4.0814\n",
      "  Step  17: █████████████████ 3.3957\n",
      "  Step  25: █████████ 2.3216\n",
      "  Step  33: ███ 1.4699\n",
      "  Step  41: ███ 1.5346\n",
      "  Step  49: ██ 1.3758\n",
      "  Step  57: ██ 1.3373\n",
      "  Step  65:  1.1149\n",
      "  Step  73: █ 1.2524\n"
     ]
    }
   ],
   "source": [
    "# Validate: Plot training loss\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Training Progress\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show loss trend (first, middle, last)\n",
    "print(f\"\\nLoss at different points:\")\n",
    "print(f\"  Start (step 1):     {training_losses[0]:.4f}\")\n",
    "print(f\"  Middle (step {len(training_losses)//2}):   {training_losses[len(training_losses)//2]:.4f}\")\n",
    "print(f\"  End (step {len(training_losses)}):    {training_losses[-1]:.4f}\")\n",
    "\n",
    "# Check if loss decreased\n",
    "loss_decreased = training_losses[-1] < training_losses[0]\n",
    "print(f\"\\nLoss decreased during training: {'✓ Yes' if loss_decreased else '✗ No'}\")\n",
    "\n",
    "# Simple ASCII plot\n",
    "print(f\"\\nLoss curve (every 10 steps):\")\n",
    "max_loss = max(training_losses)\n",
    "min_loss = min(training_losses)\n",
    "for i in range(0, len(training_losses), max(1, len(training_losses)//10)):\n",
    "    bar_len = int(30 * (training_losses[i] - min_loss) / (max_loss - min_loss + 1e-6))\n",
    "    print(f\"  Step {i+1:3d}: {'█' * bar_len} {training_losses[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Checkpoint\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Checkpoint Saved\n",
      "============================================================\n",
      "\n",
      "Checkpoint saved to: ../checkpoints/demo_model.pt\n",
      "File size: 8337.3 KB\n",
      "\n",
      "Checkpoint contents:\n",
      "  model_state_dict: [present]\n",
      "  optimizer_state_dict: [present]\n",
      "  scheduler_state_dict: [present]\n",
      "  global_step: [present]\n",
      "  final_loss: [present]\n",
      "  config: {'vocab_size': 104, 'd_model': 128, 'n_heads': 4, 'n_layers': 2, 'd_ff': 256, 'dropout': 0.1}\n",
      "\n",
      "✓ Checkpoint saved and verified!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = '../checkpoints/demo_model.pt'\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'global_step': global_step,\n",
    "    'final_loss': training_losses[-1],\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': d_model,\n",
    "        'n_heads': n_heads,\n",
    "        'n_layers': n_layers,\n",
    "        'd_ff': d_ff,\n",
    "        'dropout': dropout,\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "# Validate checkpoint\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Checkpoint Saved\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCheckpoint saved to: {checkpoint_path}\")\n",
    "print(f\"File size: {os.path.getsize(checkpoint_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Verify we can load it\n",
    "loaded = torch.load(checkpoint_path, weights_only=False)\n",
    "print(f\"\\nCheckpoint contents:\")\n",
    "for key in loaded:\n",
    "    if key == 'config':\n",
    "        print(f\"  {key}: {loaded[key]}\")\n",
    "    else:\n",
    "        print(f\"  {key}: [present]\")\n",
    "\n",
    "print(f\"\\n✓ Checkpoint saved and verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. **Prepare data** - Create parallel sentence pairs for translation\n",
    "2. **Tokenize** - Build vocabulary and convert text to token IDs\n",
    "3. **Create DataLoader** - Batch and pad sequences efficiently\n",
    "4. **Build model** - Construct a Transformer with proper hyperparameters\n",
    "5. **Setup training** - Configure optimizer, scheduler, and loss function\n",
    "6. **Train** - Run the training loop with gradient clipping\n",
    "7. **Save** - Store the trained model checkpoint\n",
    "\n",
    "Each step was validated to ensure correctness. The training loss should decrease over epochs, indicating the model is learning.\n",
    "\n",
    "For production training:\n",
    "- Use larger d_model (512) and more layers (6)\n",
    "- Use BPE tokenizer with larger vocabulary (~37K)\n",
    "- Train on WMT14 or similar large datasets\n",
    "- Use dynamic batching based on max_tokens\n",
    "- Train for 100K+ steps with warmup_steps=4000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
