{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training Tutorial\n",
    "\n",
    "This notebook walks you through training a Transformer model step by step.\n",
    "Each step includes validation to verify correctness.\n",
    "\n",
    "## Overview\n",
    "1. Setup and imports\n",
    "2. Prepare sample data\n",
    "3. Create tokenizer and vocabulary\n",
    "4. Create dataset and dataloader\n",
    "5. Build the Transformer model\n",
    "6. Setup optimizer, scheduler, and loss function\n",
    "7. Training loop\n",
    "8. Save checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "First, let's import all necessary modules and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory to path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import our Transformer implementation\n",
    "from src import Transformer\n",
    "from src.tokenizer import SimpleTokenizer, pad_sequences, PAD_ID, BOS_ID, EOS_ID\n",
    "from src.data import TranslationDataset, TranslationCollator, create_translation_dataloader\n",
    "from src.scheduler import TransformerScheduler, get_lr_at_step\n",
    "from src.label_smoothing import LabelSmoothingLoss\n",
    "from src.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Sample Data\n",
    "\n",
    "We'll use a small English-to-German translation dataset for demonstration.\n",
    "In production, you would use WMT14 or similar datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parallel sentences (English -> German)\n",
    "src_sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"Hello world\",\n",
    "    \"How are you today\",\n",
    "    \"I love machine learning\",\n",
    "    \"The weather is nice\",\n",
    "    \"Good morning everyone\",\n",
    "    \"This is a test\",\n",
    "    \"The dog runs fast\",\n",
    "    \"She reads a book\",\n",
    "    \"We learn together\",\n",
    "    \"The sun is shining\",\n",
    "    \"Birds fly in the sky\",\n",
    "    \"I drink coffee\",\n",
    "    \"He plays guitar\",\n",
    "    \"They study hard\",\n",
    "    \"The flower is beautiful\",\n",
    "]\n",
    "\n",
    "tgt_sentences = [\n",
    "    \"Die Katze saß auf der Matte\",\n",
    "    \"Hallo Welt\",\n",
    "    \"Wie geht es dir heute\",\n",
    "    \"Ich liebe maschinelles Lernen\",\n",
    "    \"Das Wetter ist schön\",\n",
    "    \"Guten Morgen allerseits\",\n",
    "    \"Das ist ein Test\",\n",
    "    \"Der Hund läuft schnell\",\n",
    "    \"Sie liest ein Buch\",\n",
    "    \"Wir lernen zusammen\",\n",
    "    \"Die Sonne scheint\",\n",
    "    \"Vögel fliegen am Himmel\",\n",
    "    \"Ich trinke Kaffee\",\n",
    "    \"Er spielt Gitarre\",\n",
    "    \"Sie lernen fleißig\",\n",
    "    \"Die Blume ist wunderschön\",\n",
    "]\n",
    "\n",
    "# Validate: Show first 3 lines\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: First 3 sentence pairs\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(3):\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"  Source (EN): {src_sentences[i]}\")\n",
    "    print(f\"  Target (DE): {tgt_sentences[i]}\")\n",
    "\n",
    "print(f\"\\n✓ Total sentence pairs: {len(src_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Tokenizer and Build Vocabulary\n",
    "\n",
    "We use a SimpleTokenizer for this demo. In production, use BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer and build vocabulary from all sentences\n",
    "tokenizer = SimpleTokenizer()\n",
    "all_sentences = src_sentences + tgt_sentences\n",
    "tokenizer.build_vocab(all_sentences)\n",
    "\n",
    "# Validate tokenizer\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Tokenizer\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"\\nSpecial tokens:\")\n",
    "print(f\"  PAD_ID = {tokenizer.pad_id}\")\n",
    "print(f\"  UNK_ID = {tokenizer.unk_id}\")\n",
    "print(f\"  BOS_ID = {tokenizer.bos_id}\")\n",
    "print(f\"  EOS_ID = {tokenizer.eos_id}\")\n",
    "\n",
    "# Test encoding/decoding on first 3 sentences\n",
    "print(f\"\\nEncoding first 3 sentences:\")\n",
    "for i in range(3):\n",
    "    tokens = tokenizer.encode(src_sentences[i], add_bos=True, add_eos=True)\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"\\n  Sentence {i+1}: '{src_sentences[i]}'\")\n",
    "    print(f\"  Token IDs:   {tokens}\")\n",
    "    print(f\"  Decoded:     '{decoded}'\")\n",
    "\n",
    "print(f\"\\n✓ Tokenizer is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Dataset and DataLoader\n",
    "\n",
    "Wrap our data in PyTorch Dataset and DataLoader for efficient batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = TranslationDataset(\n",
    "    src_data=src_sentences,\n",
    "    tgt_data=tgt_sentences,\n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    add_bos=True,\n",
    "    add_eos=True,\n",
    ")\n",
    "\n",
    "# Create dataloader with collator for padding\n",
    "collator = TranslationCollator(pad_id=tokenizer.pad_id)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "# Validate: Show first batch\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: DataLoader\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Batch size: 4\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# Get one batch and inspect\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nFirst batch contents:\")\n",
    "print(f\"  src shape: {sample_batch['src'].shape}\")\n",
    "print(f\"  tgt shape: {sample_batch['tgt'].shape}\")\n",
    "print(f\"  src_mask shape: {sample_batch['src_mask'].shape}\")\n",
    "\n",
    "# Show first 3 samples from batch\n",
    "print(f\"\\nFirst 3 samples in batch:\")\n",
    "for i in range(min(3, sample_batch['src'].size(0))):\n",
    "    src_ids = sample_batch['src'][i].tolist()\n",
    "    tgt_ids = sample_batch['tgt'][i].tolist()\n",
    "    print(f\"\\n  Sample {i+1}:\")\n",
    "    print(f\"    src IDs: {src_ids}\")\n",
    "    print(f\"    tgt IDs: {tgt_ids}\")\n",
    "    print(f\"    src decoded: '{tokenizer.decode(src_ids)}'\")\n",
    "    print(f\"    tgt decoded: '{tokenizer.decode(tgt_ids)}'\")\n",
    "\n",
    "print(f\"\\n✓ DataLoader is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Transformer Model\n",
    "\n",
    "Create the Transformer model with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters (smaller for demo)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 128      # Model dimension (paper uses 512)\n",
    "n_heads = 4        # Attention heads (paper uses 8)\n",
    "n_layers = 2       # Encoder/decoder layers (paper uses 6)\n",
    "d_ff = 256         # FFN dimension (paper uses 2048)\n",
    "dropout = 0.1\n",
    "max_seq_len = 100\n",
    "\n",
    "# Create model\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_encoder_layers=n_layers,\n",
    "    n_decoder_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout,\n",
    "    max_seq_len=max_seq_len,\n",
    "    pad_idx=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Validate model\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Model Architecture\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel hyperparameters:\")\n",
    "print(f\"  vocab_size: {vocab_size}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  n_layers: {n_layers}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"  dropout: {dropout}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\nTesting forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src = sample_batch['src'].to(device)\n",
    "    tgt = sample_batch['tgt'].to(device)\n",
    "    \n",
    "    # Use tgt[:-1] as input (teacher forcing)\n",
    "    tgt_input = tgt[:, :-1]\n",
    "    logits = model(src, tgt_input)\n",
    "    \n",
    "    print(f\"  Input src shape: {src.shape}\")\n",
    "    print(f\"  Input tgt shape: {tgt_input.shape}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    print(f\"  Expected shape: (batch, tgt_len-1, vocab_size) = ({src.size(0)}, {tgt_input.size(1)}, {vocab_size})\")\n",
    "\n",
    "print(f\"\\n✓ Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup Optimizer, Scheduler, and Loss Function\n",
    "\n",
    "Configure training components as described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer (Adam with paper's beta values)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1.0,  # Will be overridden by scheduler\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (warmup + inverse sqrt decay)\n",
    "scheduler = TransformerScheduler(\n",
    "    optimizer,\n",
    "    d_model=d_model,\n",
    "    warmup_steps=100,  # Smaller for demo (paper uses 4000)\n",
    ")\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = LabelSmoothingLoss(\n",
    "    smoothing=0.1,\n",
    "    padding_idx=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "# Validate training components\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Training Components\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"  betas: (0.9, 0.98)\")\n",
    "print(f\"  eps: 1e-9\")\n",
    "\n",
    "print(f\"\\nScheduler: TransformerScheduler\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  warmup_steps: 100\")\n",
    "\n",
    "# Show learning rate at different steps\n",
    "print(f\"\\nLearning rate schedule (first 5 steps):\")\n",
    "for step in [1, 25, 50, 100, 200]:\n",
    "    lr = get_lr_at_step(step, d_model=d_model, warmup_steps=100)\n",
    "    print(f\"  Step {step:4d}: lr = {lr:.6f}\")\n",
    "\n",
    "print(f\"\\nLoss function: LabelSmoothingLoss\")\n",
    "print(f\"  smoothing: 0.1\")\n",
    "print(f\"  padding_idx: {tokenizer.pad_id}\")\n",
    "\n",
    "# Test loss computation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src = sample_batch['src'].to(device)\n",
    "    tgt = sample_batch['tgt'].to(device)\n",
    "    tgt_input = tgt[:, :-1]\n",
    "    tgt_output = tgt[:, 1:]\n",
    "    \n",
    "    logits = model(src, tgt_input)\n",
    "    \n",
    "    # Flatten for loss\n",
    "    loss = criterion(\n",
    "        logits.contiguous().view(-1, vocab_size),\n",
    "        tgt_output.contiguous().view(-1)\n",
    "    )\n",
    "    print(f\"\\nTest loss computation:\")\n",
    "    print(f\"  Loss value: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Training components are configured correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training Loop\n",
    "\n",
    "Run the training loop and observe loss decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "log_interval = 5  # Log every N batches\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "global_step = 0\n",
    "training_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        \n",
    "        # Teacher forcing: input is tgt[:-1], target is tgt[1:]\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(src, tgt_input)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(\n",
    "            logits.contiguous().view(-1, vocab_size),\n",
    "            tgt_output.contiguous().view(-1)\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        global_step += 1\n",
    "        training_losses.append(loss.item())\n",
    "    \n",
    "    # Print epoch summary\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "print(f\"\\n✓ Training completed!\")\n",
    "print(f\"  Final loss: {training_losses[-1]:.4f}\")\n",
    "print(f\"  Total steps: {global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate: Plot training loss\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Training Progress\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show loss trend (first, middle, last)\n",
    "print(f\"\\nLoss at different points:\")\n",
    "print(f\"  Start (step 1):     {training_losses[0]:.4f}\")\n",
    "print(f\"  Middle (step {len(training_losses)//2}):   {training_losses[len(training_losses)//2]:.4f}\")\n",
    "print(f\"  End (step {len(training_losses)}):    {training_losses[-1]:.4f}\")\n",
    "\n",
    "# Check if loss decreased\n",
    "loss_decreased = training_losses[-1] < training_losses[0]\n",
    "print(f\"\\nLoss decreased during training: {'✓ Yes' if loss_decreased else '✗ No'}\")\n",
    "\n",
    "# Simple ASCII plot\n",
    "print(f\"\\nLoss curve (every 10 steps):\")\n",
    "max_loss = max(training_losses)\n",
    "min_loss = min(training_losses)\n",
    "for i in range(0, len(training_losses), max(1, len(training_losses)//10)):\n",
    "    bar_len = int(30 * (training_losses[i] - min_loss) / (max_loss - min_loss + 1e-6))\n",
    "    print(f\"  Step {i+1:3d}: {'█' * bar_len} {training_losses[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Checkpoint\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = '../checkpoints/demo_model.pt'\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'global_step': global_step,\n",
    "    'final_loss': training_losses[-1],\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': d_model,\n",
    "        'n_heads': n_heads,\n",
    "        'n_layers': n_layers,\n",
    "        'd_ff': d_ff,\n",
    "        'dropout': dropout,\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "# Validate checkpoint\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Checkpoint Saved\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCheckpoint saved to: {checkpoint_path}\")\n",
    "print(f\"File size: {os.path.getsize(checkpoint_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Verify we can load it\n",
    "loaded = torch.load(checkpoint_path, weights_only=False)\n",
    "print(f\"\\nCheckpoint contents:\")\n",
    "for key in loaded:\n",
    "    if key == 'config':\n",
    "        print(f\"  {key}: {loaded[key]}\")\n",
    "    else:\n",
    "        print(f\"  {key}: [present]\")\n",
    "\n",
    "print(f\"\\n✓ Checkpoint saved and verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. **Prepare data** - Create parallel sentence pairs for translation\n",
    "2. **Tokenize** - Build vocabulary and convert text to token IDs\n",
    "3. **Create DataLoader** - Batch and pad sequences efficiently\n",
    "4. **Build model** - Construct a Transformer with proper hyperparameters\n",
    "5. **Setup training** - Configure optimizer, scheduler, and loss function\n",
    "6. **Train** - Run the training loop with gradient clipping\n",
    "7. **Save** - Store the trained model checkpoint\n",
    "\n",
    "Each step was validated to ensure correctness. The training loss should decrease over epochs, indicating the model is learning.\n",
    "\n",
    "For production training:\n",
    "- Use larger d_model (512) and more layers (6)\n",
    "- Use BPE tokenizer with larger vocabulary (~37K)\n",
    "- Train on WMT14 or similar large datasets\n",
    "- Use dynamic batching based on max_tokens\n",
    "- Train for 100K+ steps with warmup_steps=4000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
