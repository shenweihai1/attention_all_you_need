{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# WMT14 English-German Training\n",
    "\n",
    "Production training notebook for the Transformer base model on WMT14 English-German translation task.\n",
    "\n",
    "## Configuration\n",
    "- **Model**: Base Transformer (d_model=512, n_heads=8, n_layers=6, d_ff=2048)\n",
    "- **Dataset**: WMT14 English-German (~4.5M sentence pairs)\n",
    "- **Tokenizer**: BPE with shared 37K vocabulary\n",
    "- **Training**: 100K steps, warmup 4000 steps, label smoothing 0.1\n",
    "\n",
    "## Requirements\n",
    "```bash\n",
    "pip install datasets sentencepiece sacrebleu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Our implementation\n",
    "from src import Transformer\n",
    "from src.tokenizer import Tokenizer, PAD_ID, BOS_ID, EOS_ID\n",
    "from src.data import (\n",
    "    TranslationDataset,\n",
    "    TranslationCollator,\n",
    "    create_dynamic_dataloader,\n",
    "    DynamicBatchSampler,\n",
    ")\n",
    "from src.scheduler import TransformerScheduler\n",
    "from src.label_smoothing import LabelSmoothingLoss\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Modify these settings as needed\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Model (Base Transformer from the paper)\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 6,\n",
    "    \"d_ff\": 2048,\n",
    "    \"dropout\": 0.1,\n",
    "    \"max_seq_len\": 512,\n",
    "    \n",
    "    # Tokenizer\n",
    "    \"vocab_size\": 37000,  # Shared EN-DE vocabulary\n",
    "    \n",
    "    # Training\n",
    "    \"max_steps\": 100000,\n",
    "    \"warmup_steps\": 4000,\n",
    "    \"label_smoothing\": 0.1,\n",
    "    \"max_tokens_per_batch\": 4096,  # Tokens per batch (dynamic batching)\n",
    "    \"gradient_accumulation_steps\": 4,  # Effective batch ~16K tokens\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \n",
    "    # Optimizer (Adam with paper settings)\n",
    "    \"adam_betas\": (0.9, 0.98),\n",
    "    \"adam_eps\": 1e-9,\n",
    "    \n",
    "    # Logging and checkpointing\n",
    "    \"log_steps\": 100,\n",
    "    \"eval_steps\": 2000,\n",
    "    \"save_steps\": 5000,\n",
    "    \"checkpoint_dir\": \"../checkpoints/wmt14_base\",\n",
    "    \n",
    "    # Data\n",
    "    \"max_train_samples\": None,  # Set to int for debugging (e.g., 10000)\n",
    "    \"max_val_samples\": 3000,    # Validation subset for speed\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load WMT14 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading WMT14 English-German dataset...\")\n",
    "print(\"This may take a while on first run (downloading ~1.7GB)\")\n",
    "\n",
    "# Load dataset\n",
    "wmt14 = load_dataset(\"wmt14\", \"de-en\")\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "for split, data in wmt14.items():\n",
    "    print(f\"  {split}: {len(data):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences\n",
    "def extract_sentences(dataset, max_samples=None):\n",
    "    \"\"\"Extract EN and DE sentences from WMT dataset.\"\"\"\n",
    "    en_sentences = []\n",
    "    de_sentences = []\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "        translation = example[\"translation\"]\n",
    "        en_sentences.append(translation[\"en\"])\n",
    "        de_sentences.append(translation[\"de\"])\n",
    "    \n",
    "    return en_sentences, de_sentences\n",
    "\n",
    "# Extract training data\n",
    "print(\"Extracting training sentences...\")\n",
    "train_en, train_de = extract_sentences(\n",
    "    wmt14[\"train\"], \n",
    "    max_samples=CONFIG[\"max_train_samples\"]\n",
    ")\n",
    "print(f\"Training: {len(train_en):,} sentence pairs\")\n",
    "\n",
    "# Extract validation data\n",
    "print(\"Extracting validation sentences...\")\n",
    "val_en, val_de = extract_sentences(\n",
    "    wmt14[\"validation\"],\n",
    "    max_samples=CONFIG[\"max_val_samples\"]\n",
    ")\n",
    "print(f\"Validation: {len(val_en):,} sentence pairs\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample sentence pairs:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n  [{i+1}] EN: {train_en[i][:80]}...\" if len(train_en[i]) > 80 else f\"\\n  [{i+1}] EN: {train_en[i]}\")\n",
    "    print(f\"      DE: {train_de[i][:80]}...\" if len(train_de[i]) > 80 else f\"      DE: {train_de[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Train BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "tokenizer_path = Path(CONFIG[\"checkpoint_dir\"]) / \"tokenizer.model\"\n",
    "\n",
    "if tokenizer_path.exists():\n",
    "    print(f\"Loading existing tokenizer from {tokenizer_path}\")\n",
    "    tokenizer = Tokenizer(model_path=str(tokenizer_path))\n",
    "else:\n",
    "    print(\"Training BPE tokenizer on combined EN+DE data...\")\n",
    "    print(f\"Target vocabulary size: {CONFIG['vocab_size']}\")\n",
    "    \n",
    "    # Write training data to temp file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "        # Use subset for tokenizer training (faster)\n",
    "        tokenizer_train_size = min(1000000, len(train_en))\n",
    "        for i in range(tokenizer_train_size):\n",
    "            f.write(train_en[i].strip() + \"\\n\")\n",
    "            f.write(train_de[i].strip() + \"\\n\")\n",
    "        temp_path = f.name\n",
    "    \n",
    "    print(f\"Training on {tokenizer_train_size * 2:,} sentences...\")\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer = Tokenizer.train(\n",
    "        input_files=temp_path,\n",
    "        model_prefix=str(tokenizer_path).replace('.model', ''),\n",
    "        vocab_size=CONFIG[\"vocab_size\"],\n",
    "        model_type=\"bpe\",\n",
    "        character_coverage=1.0,\n",
    "        num_threads=8,\n",
    "    )\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(temp_path)\n",
    "    print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "print(f\"\\nTokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: PAD={tokenizer.pad_id}, BOS={tokenizer.bos_id}, EOS={tokenizer.eos_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenizer\n",
    "test_sentences = [\n",
    "    \"The Transformer architecture is based on self-attention.\",\n",
    "    \"Die Transformer-Architektur basiert auf Self-Attention.\",\n",
    "]\n",
    "\n",
    "print(\"Tokenizer test:\")\n",
    "for sent in test_sentences:\n",
    "    ids = tokenizer.encode(sent, add_bos=True, add_eos=True)\n",
    "    pieces = tokenizer.encode_as_pieces(sent)\n",
    "    decoded = tokenizer.decode(ids)\n",
    "    print(f\"\\n  Input: {sent}\")\n",
    "    print(f\"  Pieces: {pieces[:10]}{'...' if len(pieces) > 10 else ''}\")\n",
    "    print(f\"  IDs: {ids[:10]}{'...' if len(ids) > 10 else ''} (len={len(ids)})\")\n",
    "    print(f\"  Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating training dataset...\")\n",
    "train_dataset = TranslationDataset(\n",
    "    src_data=train_en,\n",
    "    tgt_data=train_de,\n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_length=CONFIG[\"max_seq_len\"],\n",
    "    add_bos=True,\n",
    "    add_eos=True,\n",
    ")\n",
    "print(f\"Training dataset: {len(train_dataset):,} examples\")\n",
    "\n",
    "print(\"Creating validation dataset...\")\n",
    "val_dataset = TranslationDataset(\n",
    "    src_data=val_en,\n",
    "    tgt_data=val_de,\n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_length=CONFIG[\"max_seq_len\"],\n",
    "    add_bos=True,\n",
    "    add_eos=True,\n",
    ")\n",
    "print(f\"Validation dataset: {len(val_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders with dynamic batching\n",
    "print(f\"Creating dataloaders with max_tokens={CONFIG['max_tokens_per_batch']}...\")\n",
    "\n",
    "train_loader = create_dynamic_dataloader(\n",
    "    dataset=train_dataset,\n",
    "    max_tokens=CONFIG[\"max_tokens_per_batch\"],\n",
    "    max_sentences=128,\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pad_id=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "val_loader = create_dynamic_dataloader(\n",
    "    dataset=val_dataset,\n",
    "    max_tokens=CONFIG[\"max_tokens_per_batch\"],\n",
    "    max_sentences=128,\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pad_id=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader):,}\")\n",
    "print(f\"Validation batches: {len(val_loader):,}\")\n",
    "\n",
    "# Check first batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  src shape: {sample_batch['src'].shape}\")\n",
    "print(f\"  tgt shape: {sample_batch['tgt'].shape}\")\n",
    "print(f\"  tokens in batch: {sample_batch['src'].numel() + sample_batch['tgt'].numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Transformer model...\")\n",
    "print(f\"  d_model: {CONFIG['d_model']}\")\n",
    "print(f\"  n_heads: {CONFIG['n_heads']}\")\n",
    "print(f\"  n_layers: {CONFIG['n_layers']}\")\n",
    "print(f\"  d_ff: {CONFIG['d_ff']}\")\n",
    "print(f\"  vocab_size: {tokenizer.vocab_size}\")\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=tokenizer.vocab_size,\n",
    "    tgt_vocab_size=tokenizer.vocab_size,\n",
    "    d_model=CONFIG[\"d_model\"],\n",
    "    n_heads=CONFIG[\"n_heads\"],\n",
    "    n_encoder_layers=CONFIG[\"n_layers\"],\n",
    "    n_decoder_layers=CONFIG[\"n_layers\"],\n",
    "    d_ff=CONFIG[\"d_ff\"],\n",
    "    dropout=CONFIG[\"dropout\"],\n",
    "    max_seq_len=CONFIG[\"max_seq_len\"],\n",
    "    pad_idx=tokenizer.pad_id,\n",
    "    share_embeddings=True,  # Share embeddings between encoder and decoder\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Size: ~{total_params * 4 / 1e6:.1f} MB (fp32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "print(\"Testing forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src = sample_batch['src'].to(device)\n",
    "    tgt = sample_batch['tgt'].to(device)\n",
    "    tgt_input = tgt[:, :-1]\n",
    "    \n",
    "    logits = model(src, tgt_input)\n",
    "    print(f\"  Input src: {src.shape}\")\n",
    "    print(f\"  Input tgt: {tgt_input.shape}\")\n",
    "    print(f\"  Output logits: {logits.shape}\")\n",
    "    print(f\"  Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1.0,  # Will be controlled by scheduler\n",
    "    betas=CONFIG[\"adam_betas\"],\n",
    "    eps=CONFIG[\"adam_eps\"],\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = TransformerScheduler(\n",
    "    optimizer,\n",
    "    d_model=CONFIG[\"d_model\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    ")\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = LabelSmoothingLoss(\n",
    "    smoothing=CONFIG[\"label_smoothing\"],\n",
    "    padding_idx=tokenizer.pad_id,\n",
    ")\n",
    "\n",
    "print(\"Training components:\")\n",
    "print(f\"  Optimizer: Adam (betas={CONFIG['adam_betas']}, eps={CONFIG['adam_eps']})\")\n",
    "print(f\"  Scheduler: Transformer LR (warmup={CONFIG['warmup_steps']} steps)\")\n",
    "print(f\"  Loss: Label smoothing (eps={CONFIG['label_smoothing']})\")\n",
    "print(f\"  Gradient accumulation: {CONFIG['gradient_accumulation_steps']} steps\")\n",
    "print(f\"  Max gradient norm: {CONFIG['max_grad_norm']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch, criterion, device):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    src = batch['src'].to(device)\n",
    "    tgt = batch['tgt'].to(device)\n",
    "    \n",
    "    # Teacher forcing: input is tgt[:-1], target is tgt[1:]\n",
    "    tgt_input = tgt[:, :-1]\n",
    "    tgt_output = tgt[:, 1:]\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(src, tgt_input)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(\n",
    "        logits.contiguous().view(-1, logits.size(-1)),\n",
    "        tgt_output.contiguous().view(-1)\n",
    "    )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device, max_batches=None):\n",
    "    \"\"\"Evaluate model on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(val_loader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "            \n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        logits = model(src, tgt_input)\n",
    "        loss = criterion(\n",
    "            logits.contiguous().view(-1, logits.size(-1)),\n",
    "            tgt_output.contiguous().view(-1)\n",
    "        )\n",
    "        \n",
    "        # Count non-padding tokens\n",
    "        non_pad = (tgt_output != tokenizer.pad_id).sum().item()\n",
    "        total_loss += loss.item() * non_pad\n",
    "        total_tokens += non_pad\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, step, loss, path):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': CONFIG,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training state\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "training_history = []\n",
    "\n",
    "# Check for existing checkpoint to resume\n",
    "resume_path = Path(CONFIG[\"checkpoint_dir\"]) / \"latest_checkpoint.pt\"\n",
    "if resume_path.exists():\n",
    "    print(f\"Resuming from {resume_path}\")\n",
    "    checkpoint = torch.load(resume_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    global_step = checkpoint['step']\n",
    "    print(f\"Resumed from step {global_step}\")\n",
    "else:\n",
    "    print(\"Starting fresh training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"=\"*70)\n",
    "print(f\"Starting training for {CONFIG['max_steps']:,} steps\")\n",
    "print(f\"Gradient accumulation: {CONFIG['gradient_accumulation_steps']} steps\")\n",
    "print(f\"Effective batch size: ~{CONFIG['max_tokens_per_batch'] * CONFIG['gradient_accumulation_steps']:,} tokens\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "accumulation_loss = 0.0\n",
    "accumulation_steps = 0\n",
    "start_time = time.time()\n",
    "log_start_time = time.time()\n",
    "\n",
    "epoch = 0\n",
    "while global_step < CONFIG[\"max_steps\"]:\n",
    "    epoch += 1\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        if global_step >= CONFIG[\"max_steps\"]:\n",
    "            break\n",
    "        \n",
    "        # Forward and backward\n",
    "        loss = train_step(model, batch, criterion, device)\n",
    "        loss = loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "        loss.backward()\n",
    "        \n",
    "        accumulation_loss += loss.item()\n",
    "        accumulation_steps += 1\n",
    "        \n",
    "        # Update weights after accumulation\n",
    "        if accumulation_steps >= CONFIG[\"gradient_accumulation_steps\"]:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"max_grad_norm\"])\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Logging\n",
    "            if global_step % CONFIG[\"log_steps\"] == 0:\n",
    "                elapsed = time.time() - log_start_time\n",
    "                steps_per_sec = CONFIG[\"log_steps\"] / elapsed\n",
    "                current_lr = scheduler.get_last_lr()[0]\n",
    "                \n",
    "                print(f\"Step {global_step:6d} | \"\n",
    "                      f\"Loss: {accumulation_loss:.4f} | \"\n",
    "                      f\"LR: {current_lr:.2e} | \"\n",
    "                      f\"Speed: {steps_per_sec:.1f} steps/s\")\n",
    "                \n",
    "                training_history.append({\n",
    "                    'step': global_step,\n",
    "                    'loss': accumulation_loss,\n",
    "                    'lr': current_lr,\n",
    "                })\n",
    "                \n",
    "                log_start_time = time.time()\n",
    "            \n",
    "            # Evaluation\n",
    "            if global_step % CONFIG[\"eval_steps\"] == 0:\n",
    "                val_loss = evaluate(model, val_loader, criterion, device)\n",
    "                print(f\"  >> Validation loss: {val_loss:.4f}\")\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_path = Path(CONFIG[\"checkpoint_dir\"]) / \"best_model.pt\"\n",
    "                    save_checkpoint(model, optimizer, scheduler, global_step, val_loss, best_path)\n",
    "                    print(f\"  >> New best model saved!\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if global_step % CONFIG[\"save_steps\"] == 0:\n",
    "                ckpt_path = Path(CONFIG[\"checkpoint_dir\"]) / f\"checkpoint_step_{global_step}.pt\"\n",
    "                save_checkpoint(model, optimizer, scheduler, global_step, accumulation_loss, ckpt_path)\n",
    "                \n",
    "                # Also save as latest\n",
    "                save_checkpoint(model, optimizer, scheduler, global_step, accumulation_loss, resume_path)\n",
    "                print(f\"  >> Checkpoint saved: {ckpt_path.name}\")\n",
    "            \n",
    "            # Reset accumulation\n",
    "            accumulation_loss = 0.0\n",
    "            accumulation_steps = 0\n",
    "    \n",
    "    print(f\"\\n--- Epoch {epoch} completed ---\\n\")\n",
    "\n",
    "# Final save\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\"*70)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Total steps: {global_step:,}\")\n",
    "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save final model\n",
    "final_path = Path(CONFIG[\"checkpoint_dir\"]) / \"final_model.pt\"\n",
    "save_checkpoint(model, optimizer, scheduler, global_step, accumulation_loss, final_path)\n",
    "print(f\"Final model saved to {final_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "if training_history:\n",
    "    steps = [h['step'] for h in training_history]\n",
    "    losses = [h['loss'] for h in training_history]\n",
    "    lrs = [h['lr'] for h in training_history]\n",
    "    \n",
    "    print(\"Training Summary:\")\n",
    "    print(f\"  Initial loss: {losses[0]:.4f}\")\n",
    "    print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"  Min loss: {min(losses):.4f} (step {steps[losses.index(min(losses))]})\")\n",
    "    print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Simple ASCII visualization\n",
    "    print(\"\\nLoss curve (sampled):\")\n",
    "    sample_indices = range(0, len(losses), max(1, len(losses)//20))\n",
    "    max_loss = max(losses[i] for i in sample_indices)\n",
    "    min_loss = min(losses[i] for i in sample_indices)\n",
    "    \n",
    "    for i in sample_indices:\n",
    "        normalized = (losses[i] - min_loss) / (max_loss - min_loss + 1e-8)\n",
    "        bar = 'â–ˆ' * int(normalized * 40)\n",
    "        print(f\"  Step {steps[i]:6d}: {bar} {losses[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Quick Translation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation\n",
    "model.eval()\n",
    "\n",
    "test_sentences = [\n",
    "    \"The weather is nice today.\",\n",
    "    \"I love machine learning.\",\n",
    "    \"The European Union is an economic and political union.\",\n",
    "]\n",
    "\n",
    "print(\"Translation test:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    # Encode\n",
    "    src_ids = tokenizer.encode(sent, add_bos=True, add_eos=True)\n",
    "    src_tensor = torch.tensor([src_ids], device=device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            src=src_tensor,\n",
    "            max_len=100,\n",
    "            start_token=tokenizer.bos_id,\n",
    "            end_token=tokenizer.eos_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nEN: {sent}\")\n",
    "    print(f\"DE: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook trained a base Transformer model on WMT14 English-German:\n",
    "\n",
    "1. **Dataset**: WMT14 EN-DE (~4.5M sentence pairs)\n",
    "2. **Tokenizer**: BPE with 37K shared vocabulary\n",
    "3. **Model**: Base Transformer (65M parameters)\n",
    "   - d_model=512, n_heads=8, n_layers=6, d_ff=2048\n",
    "4. **Training**: Dynamic batching, gradient accumulation, label smoothing\n",
    "\n",
    "### Checkpoints saved:\n",
    "- `checkpoints/wmt14_base/best_model.pt` - Best validation loss\n",
    "- `checkpoints/wmt14_base/final_model.pt` - Final model\n",
    "- `checkpoints/wmt14_base/tokenizer.model` - BPE tokenizer\n",
    "\n",
    "### Next steps:\n",
    "- Use `04_wmt14_inference.ipynb` to run inference and evaluate BLEU scores\n",
    "- Implement beam search for better translation quality\n",
    "- Train longer for better results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
